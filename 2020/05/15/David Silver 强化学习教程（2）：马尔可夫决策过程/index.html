

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Xiang Li">
  <meta name="keywords" content="">
  <title>David Silver 强化学习教程（2）：马尔可夫决策过程 - 李翔的个人博客</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
    
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.3.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>记录学习的点滴</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container page-header text-center fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-05-15 20:22" pubdate>
        2020年5月15日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3.3k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      34
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto" id="post">
            <!-- SEO header -->
            <h1 style="display: none">David Silver 强化学习教程（2）：马尔可夫决策过程</h1>
            
            <div class="markdown-body" id="post-body">
              <h1 id="马尔科夫过程"><a href="#马尔科夫过程" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h1><p><strong>Markov Property</strong></p>
<p>是随机过程中的概念，因俄国数学家马尔可夫的研究而得名。它表明在给定现在状态及所有过去状态情况下，只要当前状态可知， 就可以决定未来状态的条件概率分布，所有的历史信息都不再需要（即一个<strong>无记忆</strong>的随机过程）。我们称当前状态具有<strong>马尔科夫性</strong>即具备如下得特性：</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.1.PNG" srcset="/img/loading.gif"></p>
<p><strong>Markov Chain</strong></p>
<p>一个具有有限状态和持续时间，且满足马尔可夫性质的随机过程，称<strong>马尔可夫过程（Markov Process</strong>）或<strong>马尔科夫链（Markov Chain）</strong>。通常是以一个**元组&lt;S,P&gt;**表示，其中S是状态的集合，P是状态转移概率矩阵。</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.2.PNG" srcset="/img/loading.gif"></p>
<p><strong>示例——学生马尔科夫链</strong></p>
<p>David的课程中，将多次用到下面得学生马尔科夫过程作为案例来解释有关概念和计算。</p>
<img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.3.PNG" srcset="/img/loading.gif" style="zoom:80%;" />

<p>可以看到从某一状态开始，其后的过程根据状态转移矩阵存在多种可能情况，在强化学习中这样一组状态的转移过程被称为<strong>episode或者trajectory</strong></p>
<center><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.4.JPG" srcset="/img/loading.gif" style="zoom:67%;" /></center>

<p><strong>为什么在强化学习中要引入马尔可夫性</strong></p>
<p>我们在上篇文章提到了环境的状态转化模型，它可以表示为一个概率模型$P_{ss^{‘}}^{a}$。显然在真实的环境状态转移不仅和前一状态有关，还与上上个以及更早前的状态相关，这一会导致我们的环境转化模型非常复杂难以建模。因此我们需要对强化学习的环境转化模型进行简化，简化的方法就是假设状态转化的马尔科夫性。</p>
<p>马尔科夫过程（MP）被用于<strong>对完全可观测的环境进行描述</strong>，而几乎所有的强化学习问题都可以被视作为接下来要介绍的马尔可夫决策过程（MDP，即使部分可观测环境问题也可以转化为POMDP），因此理解从马尔可夫性到马尔科夫决策过程是理解强化学习问题的基础。</p>
<h1 id="马尔科夫奖励过程"><a href="#马尔科夫奖励过程" class="headerlink" title="马尔科夫奖励过程"></a>马尔科夫奖励过程</h1><p>现在我们逐渐加入强化学习中的一些要素来拓展马尔科夫过程，首先在马尔科夫过程的基础上<strong>增加了奖励R和衰减系数γ</strong>，就得到了马尔科夫奖励过程 </p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.5.PNG" srcset="/img/loading.gif"></p>
<h2 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h2><p><strong>奖励函数</strong>：需要注意一下David在这里的表述：在时间点 $t$, agent处于状态 $s$ ，而在 $t+1$ 时刻获得环境反馈的奖励 $R_{t+1}$，因为$s$ 转移过去的下一状态是随机的，因此这里对奖励值取了一次期望得到$R_s$。照此理解起来相当于<strong>离开当前状态 $s$ 获得了奖励</strong>，David指出这是本门课程的习惯表示，如果把奖励改为 <strong>$R{t+1}$</strong> 只要在表述上描述成：$t+1$时刻进入某个状态 $s^{‘}$ 获得的相应奖励即可，本质上意义是相同的。</p>
<p>下图是例子中各个状态的即时奖励情况</p>
<center><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.6.PNG" srcset="/img/loading.gif" style="zoom:80%;" /></center>

<h2 id="Discount-Factor"><a href="#Discount-Factor" class="headerlink" title="Discount Factor"></a>Discount Factor</h2><p><strong>折扣因子</strong> $\gamma$ 介于 [0, 1]区间，为什么引入衰减系数的原因在于：</p>
<ul>
<li>避免在循环或者无限的MDP过程中，产生无穷大或者无穷小的值的值函数，陷入无限循环</li>
<li>在某些领域如金融学上，立即回报可以赚取比延迟汇报更多的利息，因而更有价值</li>
<li>远期利益具有一定的不确定性，符合人类和动物更偏爱对立即利益的追求，折扣因子用来模拟这样的认知模式</li>
</ul>
<h2 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h2><p>通常译为“<strong>收益</strong>”或”<strong>回报</strong>“，定义为在一个马尔科夫奖励链上从 $t$ 时刻开始，往后所有奖励的有衰减的总和。其中衰减系数体现了对未来的奖励的重视程度。$\gamma$ 接近0，则表明趋向于“近视”性评估；$\gamma$ 接近1则表明偏重考虑远期的利益。</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.7.PNG" srcset="/img/loading.gif"></p>
<p>结合上面提到的那张学生马尔可夫链说明Return的计算</p>
<center><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.8.PNG" srcset="/img/loading.gif" style="zoom:80%;" /></center>

<p>从上图也可以理解到，收益是针对一个马尔科夫链中的<strong>某一具体的状态转移过程</strong>来说的。</p>
<h2 id="State-Value-Function"><a href="#State-Value-Function" class="headerlink" title="State Value Function"></a>State Value Function</h2><p>回报是个随机值，其随机性来源于策略本身和环境。为了评价观测到某一状态的价值，引入<strong>价值函数</strong>的定义：从该状态出发，后续所有可能的状态转移过程的return的期望</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.9.PNG" srcset="/img/loading.gif"></p>
<h2 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h2><p>根据Value Function的定义，可以将其拆分为以下两部分：</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.10.PNG" srcset="/img/loading.gif"></p>
<ul>
<li>第一部分是该状态的即时奖励期望</li>
<li>另一部分是进入下一可能状态的价值函数值的期望，可以根据状态转移矩阵的概率分布得到</li>
</ul>
<p>下图更进一步阐述了贝尔曼方程的递归性质：</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.11.PNG" srcset="/img/loading.gif"></p>
<p>以学生马尔可夫过程为例，可以得到如下的推演：</p>
<center><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.12.PNG" srcset="/img/loading.gif"  /></center>

<p>根据系统的n个状态，不难进行扩展得到整个马尔可夫奖励过程Bellman方程的表示：</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.13.PNG" srcset="/img/loading.gif"></p>
<p>这本质上是一个线性方程组，可以直接进行矩阵运算得到解析解：</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.14.PNG" srcset="/img/loading.gif"></p>
<p>可以看到直接求解的复杂度是$O(n^3)$，适用于小规模的MRP，当状态数目很大时矩阵的求逆会非常困难。大规模MRP的求解通常使用<strong>迭代</strong>算法。常用的方法包括：动态规划Dynamic Programming、蒙特卡洛评估Monte-Carlo evaluation、时序差分学习Temporal-Difference，后文会逐步讲解这些方法。</p>
<h1 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h1><p>在马尔科夫奖励过程基础上增加<strong>动作集合A</strong>，就得到了马尔科夫决定过程，它是这样的一个5元组: </p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.15.PNG" srcset="/img/loading.gif"></p>
<p>引入Action之后，最重要的区别在于：原本MRP中全部由概率表示的过程，现在我们有了更多的控制权，这个决策是由agent来决定的；另一个方面，<strong>agent的决策并不能直接决定他下一步进入哪个状态，而是由action和environment共同决定</strong>。</p>
<p>下图给出学生状态的MDP过程，需要注意<em>图中红色的文字表示的是采取的行为，而不是先前的状态名</em>。对比之前的学生MRP示例可以发现，同一个状态 $s$下采取不同的行为，得到的环境奖励 $R_s^a$ 是不一样的。</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.16.PNG" srcset="/img/loading.gif"></p>
<h2 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h2><p>策略$\pi$是一个概率分布或集合，其元素$\pi(a|s)$代表<strong>在给定状态$s$下采取可能action的可能性</strong>。</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.17.PNG" srcset="/img/loading.gif"></p>
<ul>
<li>一个策略完整定义了个体的行为方式，也就是说定义了个体在各个状态下，所采取的可能行为方式及其概率</li>
<li>在具有Markov性质的决策中，概率分布只取决于当前的状态，与历史无关</li>
<li>某一确定的Policy是与时间无关，或者说静态的，只和当前面临状态有关。但是个体可以利用算法更新Policy</li>
</ul>
<blockquote>
<p>注意策略是静态的、关于整体的概念，不随状态改变而改变。变化的是在某一个状态时，依据策略可能产生的具体行为。因为具体的行为是有一定的概率的，策略就是用来描述各个不同状态下执行各个不同行为的概率。</p>
</blockquote>
<h2 id="理解MP、MRP、MDP的联系"><a href="#理解MP、MRP、MDP的联系" class="headerlink" title="理解MP、MRP、MDP的联系"></a>理解MP、MRP、MDP的联系</h2><p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.18.PNG" srcset="/img/loading.gif"></p>
<ul>
<li>给定一个MDP和策略$\pi$</li>
<li>MDP中的状态序列可以构成一个马尔可夫过程</li>
<li>状态和奖励序列组成一个马尔可夫奖励过程</li>
<li>在这个过程中满足两个方程：<ol>
<li>在执行某个策略下，状态$s$转移到$s’$发生的概率，等于执行某一个行为的概率与该行为能使状态从$s$转移至$s’$的概率的乘积之和</li>
<li>同理，当前状态$s$下执行某一指定策略得到的即时奖励，是该策略下所有可能行为得到的奖励与该行为发生的概率的乘积之和</li>
</ol>
</li>
</ul>
<h2 id="MDP下的两种Value-Function"><a href="#MDP下的两种Value-Function" class="headerlink" title="MDP下的两种Value Function"></a>MDP下的两种Value Function</h2><p>在MDP中, 价值函数可以用来描述针对状态的价值，也可以描述某一状态下执行某一动作的价值。对应<strong>状态价值函数和动作价值价值函数</strong>(严格意义上应该叫状态-动作价值函数的简写)。</p>
<p>需要注意的是这两种<strong>价值函数的定义都是建立在确定的策略$\pi$上</strong></p>
<ol>
<li><p>基于策略$\pi$的状态价值函数 $v_\pi(s)$：表示从状态s开始，遵循策略时所获得的期望收益；反映在执行当前策略$\pi$时，个体处于状态s时的价值大小</p>
</li>
<li><p>基于策略$\pi$的动作价值函数 $q_\pi(s,a)$：遵循策略在状态$s$下执行某一具体动作a所获得的期望收益；或者说在遵循策略$\pi$时，衡量对当前状态执行行为a的价值大小。行为价值函数一般都是与某一特定的状态相对应的。</p>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.19.PNG" srcset="/img/loading.gif"></p>
<blockquote>
<p>这里需要注意的是关于状态$s$和动作$a$的对应关系，在状态价值函数中后续所有可能产生的$G_t$，受到所采取的策略$\pi$的限制（确定性策略的话期望符号其实可以取消，随机性策略仍然是做了期望加权）；而对于动作价值函数，应该理解为在当前状态$s$下强制执行动作$a$（可能依据当前的策略$\pi$并不会采取该动作，当然后续的状态转移是受到策略影响的）。</p>
</blockquote>
<p><strong>MDP两种价值函数的关系</strong></p>
<p>图中空心较大圆圈表示状态，黑色实心小圆表示的是动作本身。可以看出，在遵循策略$\pi$时，状态s的价值体现为在该状态下，遵循某一策略而采取所有可能行为的价值按行为发生概率的乘积求和</p>
<center><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.21.PNG" srcset="/img/loading.gif" style="zoom:88%;" /></center>

<p>类似的，一个行为价值函数也可以表示成状态价值函数的形式。它表明某一个状态s下采取一个行为a的价值可以分为两部分：其一是离开这个状态的立即奖励，其二是所有进入新的状态的价值与其转移概率乘积的和。</p>
<center><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.22.PNG" srcset="/img/loading.gif" style="zoom:93%;" /></center>

<h2 id="Bellman-Expectation-Equation"><a href="#Bellman-Expectation-Equation" class="headerlink" title="Bellman Expectation Equation"></a>Bellman Expectation Equation</h2><p>把贝尔曼方程与上述两类价值函数之间关系的式子相结合，就能得到MDP下的贝尔曼期望方程（Bellman Expectation Equation）</p>
<center><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.23.PNG" srcset="/img/loading.gif" style="zoom:75%;" /></center>

<center><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.24.PNG" srcset="/img/loading.gif" style="zoom:80%;" /></center>

<h2 id="MDP的最优求解"><a href="#MDP的最优求解" class="headerlink" title="MDP的最优求解"></a>MDP的最优求解</h2><p>解决强化学习问题意味着要寻找一个最优的策略让个体在与环境交互过程中获得始终比其它策略都要多的收获，一旦找到这个最优策略我们就解决了这个强化学习问题。</p>
<p><strong>Optimal Value Function</strong></p>
<p>首先需要对<strong>最优状态价值函数和最优行为价值函数</strong>给出定义：</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.25.PNG" srcset="/img/loading.gif"></p>
<p>可见最优状态价值函数/最优动作价值函数是所有策略下产生的众多状态价值函数中的最大者</p>
<p><strong>Optimal Policy</strong></p>
<ul>
<li>首先需要定义什么是最优策略;</li>
</ul>
<p>当对于任何状态$s$，遵循策略$\pi$的价值不小于遵循策略$\pi’$下的价值，则策略$\pi$优于策略$\pi’$：</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.26.PNG" srcset="/img/loading.gif"></p>
<p>对于任何MDP，下面几点成立：</p>
<ol>
<li>存在一个最优策略，比任何其他策略更好或至少相等</li>
<li>可能不止一个最优策略，如果有多个最优策略存在，那么对于任意的状态$s$，所有的最优策略有相同的最优价值函数值$v_*(s)$</li>
<li>与2同理，所有的最优策略具有相同的行为价值函数$q_*(s,a)$</li>
</ol>
<ul>
<li>寻找最优策略</li>
</ul>
<p>MDP的最优策略可以根据最优行为价值函数$q_*(s,a)$来确定，我们之前说到Bellman Expectation Equation中的策略都是关于某一状态的动作概率分布，而对于最优策略则是在某一状态下采取能获得最大action-value的动作（即argmax操作，使得原本的随机动作变成确定性动作）</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.27.PNG" srcset="/img/loading.gif"></p>
<p><strong>Bellman Optimality Equation</strong></p>
<p>当取得最优策略时，在上面提到的4个Bellman Expectation Equation需要进行改动，得到下面Bellman Optimality Equation（把里面的求状态期望改成取argmax的操作）</p>
<p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/david_course2.28.PNG" srcset="/img/loading.gif"></p>
<p>虽然MDP可以直接用方程组来直接求解简单的问题，但是更复杂的问题却没有办法求解，因此我们还需要寻找其他有效的求解强化学习的方法。下一篇讨论用动态规划的方法来求解强化学习的问题。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a>
                    
                      <a class="hover-with-bg" href="/tags/AI/">AI</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2020/05/28/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">David Silver 强化学习教程（3）：动态规划</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/05/07/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/">
                        <span class="hidden-mobile">David Silver 强化学习教程（1）：强化学习简介</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '#post-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "David Silver 强化学习教程（2）：马尔可夫决策过程&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script>

  











</body>
</html>
