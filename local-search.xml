<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>[文献阅读02]Intelligent Network Slicing for V2X Services Towards 5G</title>
    <link href="/2020/08/11/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB01-Intelligent-Network-Slicing-for-V2X-Services-Towards-5G/"/>
    <url>/2020/08/11/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB01-Intelligent-Network-Slicing-for-V2X-Services-Towards-5G/</url>
    
    <content type="html"><![CDATA[<p><strong>点击可获取原文：</strong><a href="https://arxiv.org/abs/1910.01516"><em>《Intelligent Network Slicing for V2X Services Towards 5G》</em></a><em>（IEEE Network 2019，中科院SCI期刊分区 Q1，北邮的研究团队）</em></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>摘要里大概谈了这么几点：</p><ul><li>得益于目前广泛部署的LTE基础设施，5G网络对于推进车联网发展起了至关重要的作用（即NR-V2X）</li><li>现有LTE网络满足不了严格而动态的V2X业务，克服这一问题的有效方法是网络切片。通过切片可以实现在通用物理基础架构之上构建多个逻辑网络，以支持不同的V2X业务</li><li>为缓解5G网络切片日益复杂的问题，作者探讨了AI技术在自动化网络操作的使用。具体来说，这篇文章里提出了一个V2X业务的智能网络切片架构，在这一架构里将网络功能（VNF）和多维网络资源（3C资源，即computing + communication + caching）虚拟化，并分配到不同的网络切片。为智能地实现最优的切片，包括移动数据收集和ML算法设计在内的几个关键问题将被探讨，以。然后，我们开发了一个仿真plAI能为实现网络的自动化操作发挥其优势，充分调度各项资源实现各类V2X业务的QoS需求</li><li>设计了一个仿真，来说明作者提出的智能网络切片的有效性（我比较好奇这个，综述看多了还是想落回到怎么开展仿真）</li></ul><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1. INTRODUCTION"></a>1. INTRODUCTION</h2><p>伴随自动驾驶技术和车辆数量的增长，未来会催生出大量的V2X业务，这些业务大体上可以分为：safety releted和entertainment related（感觉可以理解为车联网背景下的URLLC和eMBB业务）。这些业务在数据速率、可靠性以及时延存在等性能上着多样性的需求。例如，自动驾驶需要低于10毫秒的通信延迟，并且通信可靠性为99.999%；相比之下，娱乐业务主要关注高数据速率，对通信延迟和可靠性的容忍度较高。尽管LTE为支持V2X业务已经做了一定工作（比如3gpp在R14标准化工作里引入了LTE-V2X的概念），但LTE网络本质上还是个“one size fits all”（中文好像翻译成一刀切？）的架构，不能很好满足多样化的V2X业务要求。</p><p>5G比4G引入了很多的新技术，除了在空口上的NOMA、Massive MIMO等，网络的软件化、虚拟化对于满足各项V2X业务也是较为关键的技术。</p><ol><li>网络软件化旨在提供高度灵活的端到端通信，SDN和NFV是两项关键的技术。SDN可以提供全局视角的网络信息，以及实现可编程化的网络控制；NFV则使得网络功能和各种资源不再被限制于专用的物理设备内。通过无线网络软件化，移动网络运营商可以依据业务要求定制专用逻辑网络，以更好地保证业务的QoS需求，这种逻辑网络即网络切片。网络切片横跨接入网和核心网中功能的灵活配置。显然，网络切片的优势十分适用于车联网环境，满足不同V2X业务的差异化需求。</li><li>另外，由于车联网的高复杂性和网络拓扑的高动态性，对车联网网络环境的精准感知和快速决策响应变得非常具有挑战。与传统系统／网络设计方法相比（？这个具体是啥有人懂吗），AI技术可以自动提取网络动态并根据历史观察做出决策，无需对系统需要专业和完善的知识，可应用于无线网络的不同领域。因此，有必要利用AI技术来促进无线网络的部署和管理，实现5G网络的自动化和自进化。</li></ol><p>基于以上，为V2X业务提供高度自动的网络切片非常有前景。利用人工智能技术，我们可以智能地提取具有复杂结构和内部关联的网络切片的高层模式，而这些通过传统的基于模型的方法很难实现。因此，这篇文章的研究范围是弥补网络软件化和网络智能化之间的差距，对基于AI的5G网络片架构在车载网络V2X业务中的应用进行了初步研究，主要要回答两个问题：</p><ul><li>如何设计一种灵活的网络切片体系结构，对车辆网络中各种异构资源进行虚拟化，用来实现网络切片?</li><li>如何利用先进的AI方法定制网络切片，同时考虑到车辆网络的具体需求</li></ul><p>（INTRODUCTION看完后还是很虚，这种有关网络架构的综述性文章一般都这样，AI、SDN、VNF、网络切片等比较热门的词都会涉及到，但限于本人的知识范畴，目前还没有找到哪篇文章能把这些繁杂的概念梳理得令人眼前一亮的，接着往下看吧：）</p><h2 id="2-DIVERSE-REQUIREMENTS-OF-V2X-SERVICES"><a href="#2-DIVERSE-REQUIREMENTS-OF-V2X-SERVICES" class="headerlink" title="2. DIVERSE REQUIREMENTS OF V2X SERVICES"></a>2. DIVERSE REQUIREMENTS OF V2X SERVICES</h2><p>V2X根据通信对象不同，主要包括了Vehicle-to-Pedestrian (V2P), Vehicle-to-Vehicle (V2V), Vehicle-to-Infrastructure (V2I) 和Vehicle-to-Network (V2N)四种通信模式，如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/v2-497bb425f6cfb0cc27d6138e2dc001ef_b.jpeg" alt="img"></p><p>这篇文章的作者把车联网所支持的业务分为了以下3类，并且给出了各自QoS的对比：</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/v2-2a9e01603d8619d4cf3f4cf45706ef9f_b.jpeg" alt="img"></p><blockquote><p>(不过有一点，这篇文章里单独把自动驾驶和交通安全拆开来看是出于啥考虑？自动驾驶所涉及的不应该都和安全相关吗？看了看别的论文会有这样分的：以车辆自动驾驶为核心的交通安全类应用，和非安全应用；后者又可分为以用户体验为核心的信息服务类应用，和以协同为核心的交通效率类应用，感觉这个逻辑上来讲是不是更通点？） * 交通安全类应用：主要与车辆行驶过程中的智能化决策相关，这类应用中的大部分将能够在车辆之间发送和接收数据，而不需要它们与远程云服务器进行连接。这种通信有效地将公路上的每辆车变成其他车辆传感器的延伸，为高速公路环境中提供最佳信息； * 信息服务类应用：是蜂窝移动通信业务面向新的车辆终端的延伸，包括车载视频、车载AR/VR、车载视频通话、车载智慧家庭、路径导航等; * 交通效率类应用：构建智慧城市的重要环节，智能地监控交通运行状况，实现智能的道路监控，缓解交通环境拥堵以期提高交通效率，达到车、路、环境之间的大协同。</p></blockquote><p>不管哪一种划分的角度，面向V2X服务的智能网络切片主体上就是先划出3个大的网络切片吧？至于在每个逻辑网络内又怎么细分呢这应该是后续考虑的事情。</p><h2 id="3-INTELLIGENT-NETWORK-SLICING-ARCHITECTURE-FOR-V2X-SERVICES"><a href="#3-INTELLIGENT-NETWORK-SLICING-ARCHITECTURE-FOR-V2X-SERVICES" class="headerlink" title="3. INTELLIGENT NETWORK SLICING ARCHITECTURE FOR V2X SERVICES"></a>3. INTELLIGENT NETWORK SLICING ARCHITECTURE FOR V2X SERVICES</h2><p>这一章讨论如何设计一种具有高灵活性的智能网络切片结构。</p><p>首先，为了提供集中式的网络切片控制，提出了一种基于SDN技术的车辆网络云架构。在这种集中式的架构上，我们可以方便地构建出V2X业务的智能网络切片。在这样的体系结构能发挥两个优点：</p><ol><li>通过使用NFV，不同种类的资源是从专用的网络基础设施虚拟化，然后网络切片可以定义为一个合适的收集资源，从而进一步增加灵活性。</li><li>另一方面，在考虑车辆特性的同时，通过采用AI技术有效管理网络切片</li></ol><p>（唔，给我的大概感觉就是，SDN和云计算是用来搭建整体的网络架构，然后NFV的作用是将各项可以调度的资源抽象出来，而怎么结合车联网的特性调度资源则需要借助AI技术的分析，最终实现的目标是为各类QoS要求的V2X业务打造其独有的网络切片）</p><h3 id="A-Cloud-based-Framework-for-Vehicular-Networks"><a href="#A-Cloud-based-Framework-for-Vehicular-Networks" class="headerlink" title="A. Cloud-based Framework for Vehicular Networks"></a>A. Cloud-based Framework for Vehicular Networks</h3><p>这篇论文将基于云的车联网框架中划分两个网络域：边缘云和远端云。</p><p>边缘云显然更为重要，其功能决定了V2X业务的服务质量。服务区域囊括了单个或多个宏蜂窝覆盖区域，由于更加接近终端，可以较好地确保较低的端到端响应时间</p><p>远端云具有强大的处理能力和大量的存储资源，由于边缘云资源的有限性，边缘云会存在无法满V2X业务需求的情况，这时需要向远端云申请资源，但为此需要付出额外的信令开销，并且将付出加大响应时延的代价。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/v2-23a5092389fc98136bef34bc65a73b2e_b.jpeg" alt="img"></p><p>（不过我看有些论文，还有一层车云的概念即构成了3层的车联网架构。未来的车辆本身具有一定的资源可以使用，但考虑到移动中的车辆会使得这层所谓的云结构太不稳定了吧。不过之前看过一个论文提到个场景比较有意思：考虑在一个大型停车场里的环境下，用泊车的资源来承担计算任务。但是这些车辆的异构熟悉、私有属性，怎么允许你使用他们的资源？）</p><h3 id="B-Design-of-Intelligent-Network-Slicing-Architecture"><a href="#B-Design-of-Intelligent-Network-Slicing-Architecture" class="headerlink" title="B. Design of Intelligent Network Slicing Architecture"></a>B. Design of Intelligent Network Slicing Architecture</h3><p>上述的车联网架构，为不同V2X业务的网络切片和基于AI的网络控制奠定了基础。作者把网络切片划分为以下图的四层结构（不过我很好奇这个四层，和上面讲的那个基于云的两层结构逻辑上是怎么个对应，理解得很模糊？）</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/v2-67f4f8fd0fe3c87fe89bd67ed6e4aa36_b.jpeg" alt="img"></p><ul><li><strong>Network infrastructure virtualization layer:</strong> </li></ul><p>网络基础设施层，这一层要解决两件事：</p><ol><li>每个网络域中的不同设备的资源都被映射到资源池，这个资源池分为了3个维度的资源：communication, computing and storage三种资源（应该要借助各种底层硬件的虚拟化技术，具体怎么实现的不是很了解）</li></ol><p>\2. VNF池：VNF即虚拟化网络功能，这一层在上述的3C资源池之上。为实现不同的网络功能，不同的VNF单元要使用到数量彼此不同的3C资源。控制层可在VNF集合中选择需要的VNF组合，提供不同V2X服务奠定基础。</p><ul><li><strong>Intelligent control layer：</strong></li></ul><p>控制层主要是配置和管理网络中的切片，具体来说选择合适的VNF来定制专门的网络切片；此外，控制层也负责收集、保存与分析车联网中生成的移动数据，可以利用先进的AI算法分析高级特征。（看来是整个架构里负责智能处理的环节所在）。</p><p>而这一层需要注意的是：由于车联网环境的移动特性和高动态网络拓扑，而且网络规模通常很大。AI算法通常需要更大的移动数据量和更长的处理时间来获得满意的控制的性能。因此直接利用AI算法来处理和优化每个V2X业务QoS要求，很难保障V2X业务的实时性，也会带来相关的处理复杂性。</p><p>为了使智能控制层可以快速且高效地部署和管理网络切片，作者对控制层采用了分级控制的结构：</p><ol><li>Slicing Deployment Controller(SDCon) ：切片配置控制器。位于控制层的底层，在较大的时间尺度上运行，利用ＭＬ算法分析车联网的宏观历史信息和负责网络切片的部署（部署具体是指的啥？）</li><li>Slicing Management Controller (SMCon)：切片管理控制器，负责及时响应和实时管理每个切片中的资源。</li></ol><p>需要注意：AI技术只在切片配置控制器中(SDCon) 中应用（这里估计就是各种论文里提的AI算法具体落地的位置）</p><p>（不太能理解细节：大概就是说要根据两个时间尺度来考量切片的配置，具体每个时间尺度上做些啥，不举例子的话也很难理解）</p><ul><li><strong>Network slice layer:</strong></li></ul><p>网络切片层，正如一开始对V2X业务分类的，存在着3大类别的网络切片。（不过这一层要做啥呢？下面的控制层不是把资源分配做完了吗？这一层就是做个分类？）</p><ul><li><strong>Service customized layer:</strong> </li></ul><p>网络切片定制层，感觉是个面向上层的接口，需要定义各类V2X业务的QoS要求和优先级吧，它们会被作为控制目标来指导下面网络切片的定制工作</p><p>（这一段更多的是讲了网络切片架构的搭建思路，i只能理解个大概的脉络，具体每一项的细节怎么做，和有啥学术上的研究点，还是得去专门找具体的文章结合着来看）</p><h2 id="4-CHALLENGES-AND-SOLUTIONS-IN-INTELLIGENT-NETWORK-SLICING"><a href="#4-CHALLENGES-AND-SOLUTIONS-IN-INTELLIGENT-NETWORK-SLICING" class="headerlink" title="4. CHALLENGES AND SOLUTIONS IN INTELLIGENT NETWORK SLICING"></a>4. CHALLENGES AND SOLUTIONS IN INTELLIGENT NETWORK SLICING</h2><h3 id="A-Mobile-Data-Collection-for-Intelligent-Network-Slicing"><a href="#A-Mobile-Data-Collection-for-Intelligent-Network-Slicing" class="headerlink" title="A. Mobile Data Collection for Intelligent Network Slicing"></a>A. Mobile Data Collection for Intelligent Network Slicing</h3><ul><li>在上面所提的SDCon控制器，需要适当地收集车联网中的数据，并进行适当地预处理，以分析车联网宏观状态的动态变化。直接上图，作者把车联网中需要观测的数据给划归了两类，</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/v2-7cce6d5721748f5976e2011c8f526fcb_b.jpeg" alt="img"></p><p>对于这些数据要考虑到它们具有很强的时空关联性 spatio-temporal diversity，这给数据的收集和处理实际上带来了一定的考虑角度：</p><p>一方面：作者把车联网中数据的采集类比于拍照，在不同的地点采集数据会有区别，而同一个地点连续的数据采集，其实会有一定程度上的关联，能反映该地点的平均V2X服务需求情况</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/v2-1ad81bb20b98d25c4d023a97287290a1_b.jpeg" alt="img"></p><p>这带来的一点启发是，由于车联网的宏观状态只会在较大的时间维度上变化，因此不需要实时收集数据，每隔一段时间收集数据。实际上，在一个长期的时间尺度上，周期性地收集移动数据是合理避免冗余的方法。</p><h3 id="B-Intelligent-Control-Layer-Powered-by-Deep-Reinforcement-Learning"><a href="#B-Intelligent-Control-Layer-Powered-by-Deep-Reinforcement-Learning" class="headerlink" title="B. Intelligent Control Layer Powered by Deep Reinforcement Learning"></a>B. Intelligent Control Layer Powered by Deep Reinforcement Learning</h3><p>Intelligent Control Layer 中，对于寻找最优的网络切片部署策略，强化学习无疑是最为有效的工具</p><p><img src="https://pic3.zhimg.com/v2-2e9c7f665b445676f385a2b4d98827de_b.jpeg" alt="img"></p><p>此外，作者认为由于时间上的关联，需要结合LSTM网络来考虑。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/v2-f44389565b15944f50cb6c2392ac02ea_b.jpeg" alt="img"></p><p>（读完这一章节两个感悟：</p><ol><li>不知道这种实际的车联网数据去哪里获取，或者有啥仿真可以做到的；</li><li>好好学强化学习吧，感觉车联网里的都是拿这套方法来灌水论文）</li></ol><h2 id="5-SIMULATION-RESULTS-AND-ANALYSIS"><a href="#5-SIMULATION-RESULTS-AND-ANALYSIS" class="headerlink" title="5. SIMULATION RESULTS AND ANALYSIS"></a>5. SIMULATION RESULTS AND ANALYSIS</h2><p>最后是给了一个实验，不赘述了，感兴趣的话去原文看吧。</p>]]></content>
    
    
    <categories>
      
      <category>文献阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>V2X</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>David Silver 强化学习教程（5）：免模型的控制</title>
    <link href="/2020/06/08/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%885%EF%BC%89%EF%BC%9A%E5%85%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A7%E5%88%B6/"/>
    <url>/2020/06/08/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%885%EF%BC%89%EF%BC%9A%E5%85%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A7%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>这一讲的内容主要是在模型未知的条件下如何优化价值函数，这一过程也称作模型无关的控制。</p><p>根据优化控制过程中是否利用已有经验策略来改进我们自身的控制策略，我们可以将这种优化控制分为两类：</p><p><strong>一类是同策略学习（On-policy Learning）</strong>，其基本思想是个体已有一个策略，并且遵循这个策略进行采样，或者说采取一系列该策略下产生的行为，根据这一系列行为得到的奖励，更新状态函数，最后根据该更新的价值函数来优化策略得到较优的策略。这里，要优化的策略就是当前遵循的策略</p><p><strong>另一类是异策略学习（Off-policy Learning）</strong>: 其基本思想是，虽然个体有一个自己的策略，但是个体并不针对这个策略进行采样，而是基于另一个策略进行采样，这另一个策略可以是先前学习到的策略，也可以是人类先验知识等一些较为优化成熟的策略，通过观察基于这类策略的行为，或者说通过对这类策略进行采样，得到这类策略下的各种行为，继而得到一些奖励，然后更新价值函数，即在自己的策略形成的价值函数的基础上观察别的策略产生的行为，以此达到学习的目的。</p><h1 id="On-Policy-Monte-Carlo-Control"><a href="#On-Policy-Monte-Carlo-Control" class="headerlink" title="On-Policy Monte-Carlo Control"></a>On-Policy Monte-Carlo Control</h1><p>在本节中我们使用的主要思想仍然是动态规划的思想。忘记了的不妨去看下第三讲回顾动态规划是如何进行策略迭代的。</p><p>那么这种方法是否适用于模型未知的蒙特卡洛学习呢？答案是否定的，这其中至少存在两个问题。</p><ul><li>一是在模型未知的条件下无法知道当前状态的所有后续状态，进而无法确定在当前状态下采取怎样的行为更合适。（动态规划是需要知道某一状态的所有后续状态及状态间转移概率的）</li><li>另一个问题是，动态规划是采取的贪婪算法来改善策略，将很有可能由于没有足够的采样经验而导致产生一个并不是最优的策略。为了解决这一问题，我们需要引入一个随机机制，以一定的概率选择当前最好的策略，同时给以其它可能的行为一定的几率，这就是之前介绍强化学习时提到的$\epsilon$，数学表达式如下：</li></ul><p>解决了上述两个问题，我们最终看到蒙特卡洛控制的全貌：<strong>使用Ｑ函数进行策略评估，使用$\epsilon$-贪婪探索来改善策略</strong>。该方法最终可以收敛至最优策略。如下图所示：</p><p>在实际求解控制问题时，为了使算法可以收敛，一般ϵϵ会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。这样我们可以得到一张和动态规划类似的图：</p><h1 id="On-Policy-Temporal-Difference-Control"><a href="#On-Policy-Temporal-Difference-Control" class="headerlink" title="On-Policy Temporal-Difference Control"></a>On-Policy Temporal-Difference Control</h1><p>上一讲提到TD相比MC有很多优点：</p><ol><li>低方差（lower variance）；</li><li>可以在线实时学习；</li><li>可以学习不完整episode等。</li></ol><p>那么是否可以在控制问题上使用TD学习而不是MC学习？答案是肯定的，这就是下文要讲解的<strong>SARSA算法</strong></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>SARSA算法和动态规划法比起来，不需要环境的状态转换模型，和MC法比起来，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。但是也一个传统强化学习方法共有的问题，就是无法求解太复杂的问题，$Q(S,A)$使用一张大表来存储的，如果我们的状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA算还很不错的一种强化学习问题求解方法。</p><h1 id="Off-Policy-Learning"><a href="#Off-Policy-Learning" class="headerlink" title="Off-Policy Learning"></a>Off-Policy Learning</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Reinforcement Learning</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>David Silver 强化学习教程（4）：免模型的预测</title>
    <link href="/2020/06/02/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%884%EF%BC%89%EF%BC%9A%E5%85%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B/"/>
    <url>/2020/06/02/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%884%EF%BC%89%EF%BC%9A%E5%85%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<p>上一讲介绍了在掌握MDP细节（即时奖励和转移概率，或者说对环境建立了模型）下如何通过动态规划求解最优价值函数和最优策略。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们对于环境模型是无法知道的，这时动态规划法根本没法使用。这时候我们如何求解强化学习问题呢？这正是强化学习要解决的问题，通过直接从Agent与环境的交互来得到一个估计的最优价值函数和最优策略。</p><p>本讲的内容聚焦于策略评估也就是model-free prediction；下一讲将利用本讲的主要观念来进行model-free control进而找出最优策略，最大化Agent的奖励。</p><h1 id="Monte-Carlo-Reinforcement-Learning"><a href="#Monte-Carlo-Reinforcement-Learning" class="headerlink" title="Monte-Carlo Reinforcement Learning"></a>Monte-Carlo Reinforcement Learning</h1><h2 id="蒙特卡罗算法"><a href="#蒙特卡罗算法" class="headerlink" title="蒙特卡罗算法"></a>蒙特卡罗算法</h2><p>蒙特卡罗法并不是一种算法的名称，而是对一类随机算法的特性的概括。既然是随机算法，在采样不全时，通常不能保证找到最优解，只能说是尽量找。那么根据怎么做的“尽量”，我们可以把随机算法分成两类：</p><ul><li>蒙特卡洛算法：随机采样的越多，得到的结果<strong>越近似最优解</strong>（概率越大）；</li><li>拉斯维加斯算法：另一类随机算法的思路是采样越多，越<strong>有机会找到最优解</strong>。</li></ul><p>这两个词本身是两座著名赌城，因为赌博中体现了许多随机算法，所以借过来命名。这两类随机算法之间的选择，往往受到问题的局限。如果问题要求在有限采样内，必须给出一个解，但不要求是最优解，那就要用蒙特卡罗算法。反之，如果问题要求必须给出最优解，但对采样没有限制，那就要用拉斯维加斯算法。</p><p>关于蒙特卡罗的例子可以看看这个视频（给了5个案例）：<a href="https://www.youtube.com/watch?v=XRGquU0ZJok">蒙特卡罗 Monte Carlo</a></p><h2 id="蒙特卡罗强化学习"><a href="#蒙特卡罗强化学习" class="headerlink" title="蒙特卡罗强化学习"></a>蒙特卡罗强化学习</h2><p>蒙特卡洛强化学习指：指在不清楚MDP具体细节的情况下，直接基于某个策略从某个状态开始，让个体与环境交互经历完整的状态序列 (episode) ，某状态的价值等于在多个状态序列中以该状态算得到的所有return 的平均。这里所谓<strong>完整的状态序列 (complete episode)**并不要求起始状态一定是某一个特定的状态，但是要求个体最终进入环境认可的某一个终止状态。从这里也能看出MC的局限性在于</strong>要求MDP过程存在终止状态**</p><p><strong>Monte-Carlo Policy Evaluation 的流程</strong></p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.1.JPG" style="zoom:67%;" /><p>可以看出，预测问题的求解思路还是很简单的。核心思想就是<strong>用平均收获值代替价值（value = empirical mean return）</strong>，理论上Episode越多，结果越准确。不过有几个点可以优化考虑：</p><ul><li>第一个考虑的点在于：同样一个状态可能在一个完整的状态序列中重复出现，那么该状态的收获该如何计算？</li></ul><p>一种办法是仅把状态序列中第一次出现该状态时的收获值纳入到收获平均值的计算中，这被称为 Fist-Visit Monte-Carlo Policy Evaluation</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.2.JPG" style="zoom:80%;" /><p>另一种是针对一个状态序列中每次出现的该状态，都计算对应的收获值并纳入到收获平均值的计算中，这被称为 Every-Visit Monte-Carlo Policy</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.3.JPG" style="zoom:80%;" /><p>第二种方法比第一种的计算量要大一些，但是在完整的经历样本序列少的场景下会比第一种方法适用。</p><ul><li>另一个需要考虑的点在于：上面预测问题的求解公式里，我们有一个average的公式，意味着要保存所有该状态的收获值之和最后取平均。这样浪费了太多的存储空间。一个较好的方法是empirical mean的更新可以被写成Incremental Mean的方式，在实际操作时更新平均收获时，不需要存储所有既往收获，而是每得到一次收获，就计算其平均收获。</li></ul><p>通过下面的公式可以更好的理解（相当于在上一次数值的基础上，加上一个带学习率的残差进行更新）：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.4.JPG" style="zoom:67%;" /><p>把这个方法应用于蒙特卡洛策略评估，就得到下面的蒙特卡洛累进更新。在处理非静态问题时，使用这个方法跟踪一个实时更新的平均值是非常有用的，还可以引入参数 $\alpha$ 每得到一轮episode后更新状态价值的速度：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.5.JPG" style="zoom:80%;" /><p>以上就是蒙特卡洛学习方法的主要思想和描述，蒙特卡洛学习方法实际应用并不多。接下来着重介绍实际常用的TD学习方法。</p><h1 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal-Difference Learning"></a>Temporal-Difference Learning</h1><h2 id="TD算法流程"><a href="#TD算法流程" class="headerlink" title="TD算法流程"></a>TD算法流程</h2><p>时序差分学习简称TD学习，它的特点如下：和蒙特卡洛学习一样，它也从Episode学习，不需要了解模型本身；但是它可以学习<strong>不完整</strong>的Episode，通过<strong>自举（bootstrapping）</strong>，猜测Episode的结果，同时持续更新这个猜测。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.6.JPG" style="zoom:80%;" /><p>所谓bootstrapping，就是TD算法在估计某一个状态的价值时，用到了以前估计的值，这里很类似Bellman方程的形式。图中TD target包括了模型对价值函数的预测值和环境的实际奖励，对于这个概念的理解可以参考这个视频（第二节） <a href="https://www.bilibili.com/video/BV1BE411W7TA?p=2">https://www.bilibili.com/video/BV1BE411W7TA?p=2</a></p><h2 id="n-step-TD"><a href="#n-step-TD" class="headerlink" title="n-step TD"></a>n-step TD</h2><p>先前所介绍的TD算法实际上都是TD(0)算法，括号内的数字0表示的是在当前状态下往前多看1步，要是往前多看2步更新状态价值会怎样？这就引入了n-step的概念。</p><p>对于n步时序差分来说，和普通的时序差分的区别就在于收获的计算方式的差异。那么既然有这个n步的说法，那么n到底是多少步好呢？如何衡量n的好坏呢？我们将在下一讲讨论。</p><h2 id="MC与TD的对比"><a href="#MC与TD的对比" class="headerlink" title="MC与TD的对比"></a>MC与TD的对比</h2><p><strong>（1）</strong>TD 在知道最终结果之前可以学习，MC必须等到最终结果才能学习；TD 可以在没有终止状态时的持续进行的环境里学习。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.8.JPG" style="zoom:80%;" /><p><strong>（2）</strong> 偏差/方差分析：这也很好理解，MC的方式会经历更为长时间的状态过程，MDP的随机性会对经验平均带来大的方差；而对于TD变换的主要原因在于的即时奖励，方差自然会小但是如果即使奖励不准确会使得对价值函数的估计出现偏差</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.9.JPG" style="zoom:80%;" /><p><strong>（3）</strong> 通过比较可以看出，TD算法使用了MDP问题的马尔可夫属性，在Markov 环境下更有效；但是MC算法并不利用马尔可夫属性，通常在非Markov环境下更有效。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.10.JPG"></p><h1 id="三种强化学习算法对比"><a href="#三种强化学习算法对比" class="headerlink" title="三种强化学习算法对比"></a>三种强化学习算法对比</h1><p>Monte-Carlo, Temporal-Difference 和 Dynamic Programming 都是计算状态价值的一种方法，区别在于，前两种是在不知道Model的情况下的常用方法，这其中又以MC方法需要一个完整的Episode来更新状态价值，TD则不需要完整的Episode；DP方法则是基于Model（知道模型的运作方式）的计算状态价值的方法，它通过计算一个状态S所有可能的转移状态S’及其转移概率以及对应的即时奖励来计算这个状态S的价值。</p><p>下面的几张图直观地体现了三种算法的区别：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.11.JPG" style="zoom:67%;" /><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.12.JPG" style="zoom:67%;" /><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.13.JPG" style="zoom:67%;" /><p><strong>关于是否Bootstrapping：</strong>MC 只使用实际收获；DP和TD使用了</p><p><strong>关于是否用样本来计算:</strong> MC和TD都是应用样本来估计实际的价值函数；而DP则是利用模型直接计算得到实际价值函数，没有样本或采样之说。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.14.JPG"></p><p>从<strong>采样深度和广度</strong>两个维度来解释了四种算法的差别，多了一个穷举法。</p><ul><li>当使用单个采样，同时不走完整个Episode就是TD；</li><li>当使用单个采样但走完整个Episode就是MC；</li><li>当考虑全部样本可能性，但对每一个样本并不走完整个Episode时，就是DP；</li><li>当既考虑所有Episode又把Episode从开始到终止遍历完，就变成了穷举法。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course4.15.JPG"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Reinforcement Learning</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>David Silver 强化学习教程（3）：动态规划</title>
    <link href="/2020/05/28/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <url>/2020/05/28/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    
    <content type="html"><![CDATA[<p>从广义上讲，强化学习是序贯决策问题。在上一节，我们已经将强化学习纳入到马尔科夫决策过程MDP的框架之内，根据是否建立了环境模型（即状态转移概率），可以分为<strong>基于模型的动态规划方法和基于无模型的强化学习方法</strong>。</p><p>DP算法在增强学习领域应用十分有限，因为它们不仅要求理想的模型，同时计算量也非常大，但是在理论方面依然非常重要。 DP算法为本书后面章节的理解提供了必要的基础，强化学习所有的方法都可以看成是为了实现和DP相似的效果，只是弱化已知精确环境模型的假设或者计算量更小。</p><h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><p>动态规划算法是解决复杂问题的一个方法，算法通过把复杂问题分解为子问题，通过求解子问题进而得到整个问题的解。在解决子问题的时候，其结果通常需要存储起来被用来解决后续复杂问题。当问题具有下列特性时，通常可以考虑使用动态规划来求解：</p><ul><li>最优子结构(Optimal substructure)：意味着我们的问题可以拆分成一个个的小问题，通过解决这个小问题，最后，我们能够通过组合小问题的答案，得到大问题的答案</li><li>重叠子问题(Overlapping subproblems)：子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。</li></ul><p>马尔科夫决定过程（MDP）满足上述的两个属性，可以使用动态规划来求解MDP</p><ul><li>Bellman方程把问题递归为求解子问题，</li><li>价值函数就相当于存储了一些子问题的解，可以复用。</li></ul><p><strong>动态规划求解MDP</strong></p><p>使用动态规划解决MDP问题时，通常假设环境是有限马尔可夫决策过程。也就是说，我们假设环境的状态，动作，和奖励集合有限，且给出了他们的动态特性即转移概率</p><ul><li>动态规划应用于MDP的<strong>规划问题(planning)**而不是学习问题(learning)，我们必须</strong>对环境是完全已知的(Model-Based)**，才能做动态规划</li><li>动态规划对于prediction（评估策略）和control问题（找到最优策略可分为策略迭代和价值迭代）均能求解（定义请看第一章）</li></ul><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.1.JPG" style="zoom:67%;" /><h1 id="策略评估求解预测问题"><a href="#策略评估求解预测问题" class="headerlink" title="策略评估求解预测问题"></a>策略评估求解预测问题</h1><p>首先要介绍的是策略评估，<strong>策略评估就是给定任意策略 $\pi$怎么判断该策略到底有多好即计算其状态值函数</strong>。这个评价由基于当前策略的值函数 $v_{\pi}(s)$衡量。下面是整个流程：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.2.JPG" alt="david_course3.2" style="zoom: 80%;" /><ol><li><p>给定要评估的策略 $\pi$ 和MDP中所有状态的 value function $v_1$，一般是全部置为0。这个初始化不会影响迭代结果，并且它不会影响收敛速率。</p></li><li><p>如果要获得状态 $s$ 的价值函数，需要看在该状态下通过策略 $\pi$ 其状态能转移到了哪些后续状态式 $s^{‘}$ ；而在具体要计算的时候，利用第 $k$ 步迭代得到的这些后续状态 $s^{‘}$ 的价值函数，带入Bellman Expectation Equation，得到新一轮 $k+1$ 步迭代的价值函数</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.3.JPG" style="zoom:67%;" /></li><li><p>反复第2步一直到价值函数值恒定不变，这个迭代过程的结果 $v_{\pi}$ 就是对当前策略的评估</p></li></ol><p><strong>示例：Evaluating a Random Policy in the Small Gridworld</strong></p><p>在这节课程中用到的例子是一个最快达到网格中的灰色点，我们从最开始的随机策略开始，对其进行policy evaluation</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.4.JPG"></p><ul><li>状态空间 S：图中灰色方格所示两个位置为终止状态</li><li>行为空间 A：{n, e, s, w} 对于任何非终止状态可以有向北、东、南、西移动四个行为</li><li>转移概率 P：任何试图离开方格世界的动作其位置将不会发生改变，其余条件下将100%地转移到动作指向的状态（即在边界格子时，任何试图冲出边界的行为都会维持在原地，其他情况下移动不会出现打滑走对角线的可能）</li><li>即时奖励 R：任何在非终止状态间的转移得到的即时奖励均为-1，进入终止状态即时奖励为0</li><li>衰减系数 $\gamma$：1</li><li>初始化为随机策略，对于任何非终止状态可以等概率朝4个方向移动，目标是最快达到灰色格子的状态</li></ul><p>迭代过程的详细步骤可以参考知乎叶强大佬的文章： <a href="https://zhuanlan.zhihu.com/p/28084990">迭代法评估4*4方格世界下的随机策略</a></p><p><img src="https://pic3.zhimg.com/v2-0082e5876e2ea59bdcedc7912bb15a3a_b.jpg"></p><p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p><blockquote><p>除了通过迭代进行策略评估外，这个视频还提到了解析解的方法，感兴趣的可以参考<a href="https://www.bilibili.com/video/BV1nV411k7ve?p=3">https://www.bilibili.com/video/BV1nV411k7ve?p=3</a></p></blockquote><h1 id="策略迭代求解控制问题"><a href="#策略迭代求解控制问题" class="headerlink" title="策略迭代求解控制问题"></a>策略迭代求解控制问题</h1><p>现在我们正式介绍最优策略的解法之一，<strong>Policy Iteration策略迭代</strong>，它包括上面介绍的策略评估（policy evaluation）和策略改进（policy improvement）两个步骤。</p><p>现在我们已经通过策略评估确定了一个确定性策略$\pi$的状态价值函数，那么接下来的问题就是：是否可以找到一个更好的策略$\pi^{‘}$。最直接的办法就是对$\pi^{‘}$也进行策略评估得到状态价值函数，但这本身是个很耗时的工作。</p><p>下面要介绍的策略改进定理提供了更加简洁的思路：我们无需对新的策略$\pi^{‘}$直接求价值函数，而是考虑在状态 $s$下强制执行动作 $a=\pi^{‘}(s)$（注意在当前策略$\pi$下并不一定会采取这个动作$a$），然后遵从现有的策略$\pi$并计算出若$q{\pi}(s,\pi^{‘}(s))$的值。若相比于$V_{\pi}(s)$更大，新的策略事实上总体来说也会比较好。</p><p>我们一般采取贪婪策略提升的方法，即新的确定性策略 $\pi^{‘}$是根据当前状态$s$下所有动作中，贪婪地选使得后继状态价值增加最多的行为（即利用评估得到的$V_{\pi}(s)$求出$q{\pi}(s,\pi(s))$并从中选出最大的一项更新策略）</p><p>下面图展示Policy Iteration的过程：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.8.JPG" style="zoom:67%;" /><p>现在我们从理论的角度，来给出Policy Iteration一定会收敛到最优值函数和策略的证明：</p><ul><li>采取贪婪策略提升的方法得到新的策略</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.9.JPG"></p><ul><li>在新的策略 $\pi^{‘}$ 下，可以得到下述的不等式关系</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.10.JPG"></p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.11.JPG"></p><ul><li>当policy improvement 停止的时候，上述的不等式关系变成全等式。此时也意味着bellman optimal Equation 被满足。找到了最优的策略</li></ul><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.12.JPG" style="zoom:67%;" /><p>在small gridworld的例子中结合贪婪策略提升的方法，可以看出策略的不断提升</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.5.JPG" style="zoom:67%;" /><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.6.JPG" style="zoom:67%;" /><p>很多时候，<strong>策略的更新较早就收敛至最优策略，而状态价值的收敛要慢很多</strong>，是否有必要一定要迭代计算直到状态价值得到收敛呢？</p><p>从上面方格的例子中，我们可以看出来$k=3$的时候，策略已经达到最优，虽然此时还没有满足Bellman Optimal Equation的条件，但是最优的policy已经出来了。所以，在这之后我们做的一次次迭代实际上是无用功，Modified Policy Iteration希望解决的就是砍掉这些无用的迭代的过程，我们有以下解决方案：</p><ul><li>引入变量 $\epsilon $ 作为停止条件，在精度允许范围内即可结束</li><li>设定一个固定的策略评估次数$k$，策略评估在$k$次迭代之后就截至。事实上当$k=1$的时候，Policy Iteration算法就变成了接下来介绍的Value Iteration</li></ul><h1 id="价值迭代求解控制问题"><a href="#价值迭代求解控制问题" class="headerlink" title="价值迭代求解控制问题"></a>价值迭代求解控制问题</h1><p>策略迭代存在一个问题，就在于<strong>策略迭代每次迭代都需要进行策略评估，主要时间都花费在策略评估上，而策略评估本身可能需要多次迭代才能收敛</strong>，对一个简单的问题来说，在策略评估上花费的时间不算长；但对复杂的问题来说，这个步骤的时间实在有些长。那么是否可以提前结束策略评估呢？这就引出了下面要介绍的价值迭代。</p><p><strong>最优策略原则</strong></p><p>一个最优的策略，我们可以从两步来思考：</p><ul><li>从状态 $s$ 到下一个状态 $s^{‘}$，采取了最优的动作</li><li>后继状态每一步都按照最优的policy去做，那么我最后的结果就是最优的</li></ul><p>一个策略能够使得状态s获得最优价值，当且仅当：<strong>对于从状态s可以到达的任何状态s’，该策略能够使得状态s’的价值是最优价值：</strong></p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.13.JPG" style="zoom:80%;" /><p>我们可以知道我们期望的最终（goal）状态的位置以及反推需要明确的状态间关系，认为它是一个确定性的价值迭代<strong>。</strong>因此，我们可以把问题分解成一些列的子问题，<strong>从最终目标状态开始分析，逐渐往回推，直至推至所有状态。</strong></p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.14.JPG" style="zoom:67%;" /><p><strong>价值迭代流程</strong></p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.16.JPG" style="zoom:67%;" /><p>有关策略迭代和价值迭代的对比：<a href="https://zhuanlan.zhihu.com/p/26699028">策略迭代和价值迭代</a></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>我们将这节课学到的一种Prediction方法和两种Control的方法信息总结一下：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.17.JPG" style="zoom:67%;" /><p>上图中时间复杂度的分析，$m$为可选动作，$n$为MDP中的可能状态。</p><p>进一步我们对比下价值迭代和策略迭代，这里借用Stackoverflow的一张图：</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course3.18.JPG"></p><p>DP方法的一个特点是：所有的更新都依赖其后继状态的估计值，这称为自举（bootstrapping）。另一个特点就是开篇提到的需要精确知道环境模型。</p><p>对于非常大的问题，DP可能不实用，但与其他解决MDP的方法相比，DP方法实际上非常有效。在接下来的两节笔记中，会介绍不需要模型也不需要自举的方法——蒙特卡洛法；不需要模型但是需要自举的方法——时间差分法。</p><h1 id="补充内容：异步动态规划算法"><a href="#补充内容：异步动态规划算法" class="headerlink" title="补充内容：异步动态规划算法"></a>补充内容：异步动态规划算法</h1><p>在前几节我们讲的都是同步动态规划算法，即每轮迭代我会计算出所有的状态价值并保存起来，在下一轮中，我们使用这些保存起来的状态价值来计算新一轮的状态价值。</p><p>另一种动态规划求解是异步动态规划算法，在这些算法里，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性的更新部分状态的价值，这类算法有自己的一些独特优势，当然有额会有一些额外的代价。</p><p>常见的异步动态规划算法有三种：</p><ul><li>第一种是原位动态规划 (in-place dynamic programming)， 此时我们不会另外保存一份上一轮计算出的状态价值。而是即时计算即时更新。这样可以减少保存的状态价值的数量，节约内存。代价是收敛速度可能稍慢。</li><li>第二种是优先级动态规划 (prioritised sweeping)：该算法对每一个状态进行优先级分级，优先级越高的状态其状态价值优先得到更新。通常使用贝尔曼误差来评估状态的优先级，贝尔曼误差即新状态价值与前次计算得到的状态价值差的绝对值。这样可以加快收敛速度，代价是需要维护一个优先级队列。</li><li>第三种是实时动态规划 (real-time dynamic programming)：实时动态规划直接使用个体与环境交互产生的实际经历来更新状态价值，对于那些个体实际经历过的状态进行价值更新。这样个体经常访问过的状态将得到较高频次的价值更新，而与个体关系不密切、个体较少访问到的状态其价值得到更新的机会就较少。收敛速度可能稍慢</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Reinforcement Learning</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>David Silver 强化学习教程（2）：马尔可夫决策过程</title>
    <link href="/2020/05/15/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89%EF%BC%9A%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
    <url>/2020/05/15/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89%EF%BC%9A%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="马尔科夫过程"><a href="#马尔科夫过程" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h1><p><strong>Markov Property</strong></p><p>是随机过程中的概念，因俄国数学家马尔可夫的研究而得名。它表明在给定现在状态及所有过去状态情况下，只要当前状态可知， 就可以决定未来状态的条件概率分布，所有的历史信息都不再需要（即一个<strong>无记忆</strong>的随机过程）。我们称当前状态具有<strong>马尔科夫性</strong>，用下面的公式化描述来说明马尔科夫性：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.1.PNG" style="zoom:75%;" /><p><strong>Markov Chain</strong></p><p>对于一组状态和时间都有限，且满足马尔可夫性质的随机过程，称<strong>马尔可夫过程（Markov Process</strong>）或<strong>马尔科夫链（Markov Chain）</strong>。通过一个**元组&lt;S,P&gt;**表示，其中S是状态的集合，P是状态转移概率矩阵。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.2.PNG" style="zoom:75%;" /><p><strong>示例——学生马尔科夫链</strong></p><p>David的课程中，将多次用到下面这个学生马尔科夫过程的案例，来解释有关概念和计算。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.3.PNG" style="zoom: 80%;" /><p>可以看到从某一状态开始，其后的过程根据状态转移矩阵存在多种可能情况，在强化学习中这被称为<strong>episode或者trajectory</strong></p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.4.JPG" style="zoom: 67%;" /><p>** 为什么在强化学习中要引入马尔可夫性**</p><p>我们在上节提到了环境的状态转化模型，它可以表示为一个概率模型$P_{ss^{‘’}}^{a}$。显然在真实的环境状态转移不仅和前一状态有关，还与上上个以及更早前的状态相关，这一会导致我们的环境转化模型非常复杂难以建模。因此我们需要在强化学习的环境转移模型中进行简化。这个办法就是假设状态转化的马尔科夫性。</p><p>马尔科夫过程（MP）被用于<strong>对完全可观测的环境进行描述</strong>，而几乎所有的强化学习问题都可以被视作为接下来要介绍的马尔可夫决策过程（MDP，即使部分可观测环境问题也可以转化为POMDP），因此理解从马尔可夫性到马尔科夫决策过程是理解强化学习问题的基础。</p><h1 id="马尔科夫奖励过程"><a href="#马尔科夫奖励过程" class="headerlink" title="马尔科夫奖励过程"></a>马尔科夫奖励过程</h1><p>现在我们逐渐加入强化学习中的一些要素来拓展马尔科夫过程，首先在马尔科夫过程的基础上<strong>增加了奖励R和衰减系数γ</strong>，就得到了马尔科夫奖励过程 </p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.5.PNG" style="zoom:75%;" /><h2 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h2><p><strong>奖励函数</strong>：需要注意一下David在这里的表述：在时间点 $t$, agent处于状态 $s$ ，而在 $t+1$ 时刻获得环境反馈的奖励 $R_{t+1}$，因为$s$ 转移过去的下一状态是随机的，因此这里对奖励值取了一次期望得到$R_s$。照此理解起来相当于<strong>离开当前状态 $s$ 获得了奖励</strong>，David指出这是本门课程的习惯表示，如果把奖励改为 <strong>$R{t+1}$</strong> 只要在表述上描述成：$t+1$时刻进入某个状态 $s^{‘}$ 获得的相应奖励即可，本质上意义是相同的。</p><p>下图是例子中各个状态的即时奖励情况</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.6.PNG" style="zoom: 80%;" /><h2 id="Discount-Factor"><a href="#Discount-Factor" class="headerlink" title="Discount Factor"></a>Discount Factor</h2><p><strong>折扣因子</strong> $\gamma$ 介于 [0, 1]区间，David列举了不少理由来解释为什么引入衰减系数：</p><ul><li>避免在循环或者无限的MDP过程中，产生无穷大或者无穷小的值的值函数，陷入无限循环</li><li>金融学上，立即回报可以赚取比延迟汇报更多的利息，因而更有价值</li><li>远期利益具有一定的不确定性，符合人类和动物更偏爱对立即利益的追求，折扣因子用来模拟这样的认知模式</li></ul><h2 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h2><p>译为**“收益”或”回报”**，定义为在一个马尔科夫奖励链上从 $t$ 时刻开始，往后所有奖励的有衰减的总和。其中衰减系数体现了对未来的奖励的重视程度。$\gamma$ 接近0，则表明趋向于“近视”性评估；$\gamma$ 接近1则表明偏重考虑远期的利益。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.7.PNG" style="zoom:75%;" /><p>结合上面提到的那张学生马尔可夫链说明Return的计算</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.8.PNG" style="zoom:75%;" /><p>从上图也可以理解到，收益是针对一个马尔科夫链中的<strong>某一具体的状态转移过程</strong>来说的。</p><h2 id="State-Value-Function"><a href="#State-Value-Function" class="headerlink" title="State Value Function"></a>State Value Function</h2><p>回报是个随机值，其随机性来源于策略本身和环境。为了评价观测到某一状态的价值，引入价值函数的定义：<strong>从该状态出发，后续所有可能的状态转移过程的return的期望</strong></p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.9.PNG" style="zoom:75%;" /><h2 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h2><p>根据Value Function的定义，可以将其拆分为以下两部分：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.10.PNG" style="zoom:67%;" /><ul><li>第一部分是该状态的即时奖励期望</li><li>另一部分是进入下一可能状态的价值函数值的期望，可以根据状态转移矩阵的概率分布得到</li></ul><p>下图很好地阐述了贝尔曼方程的过程：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.11.PNG" style="zoom:75%;" /><p>以学生马尔可夫过程为例，可以得到如下的推演：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.12.PNG" style="zoom: 80%;" /><p>根据系统的n个状态，不难得到整个马尔可夫奖励过程Bellman方程的矩阵形式：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.13.PNG" style="zoom: 67%;" /><p>这本质上是一个线性方程组，可以直接得到解析解：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.14.PNG" style="zoom:75%;" /><p>可以看到直接求解的复杂度是$O(n^3)$，仅适用于小规模的MRP，当状态数目很大时矩阵的求逆会非常困难。大规模MRP的求解通常使用迭代法。常用的迭代方法有：动态规划Dynamic Programming、蒙特卡洛评估Monte-Carlo evaluation、时序差分学习Temporal-Difference，后文会逐步讲解这些方法。</p><h1 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h1><p>在马尔科夫奖励过程基础上增加<strong>动作集合A</strong>，就得到了马尔科夫决定过程，它是这样的一个5元组: </p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.15.PNG" style="zoom:75%;" /><p>引入Action之后，最重要的区别在于：原本MRP中全部由概率表示的过程，现在我们有了更多的控制权，这个决策是由agent来决定的；另一个方面，<strong>agent的决策并不能直接决定他下一步进入哪个状态，而是由action和environment共同决定</strong>。</p><p>下图给出学生状态的MDP过程，图中红色的文字表示的是采取的行为，而不是先前的状态名。对比之前的学生MRP示例可以发现，同一个状态 $s$下采取不同的行为，得到的环境奖励 $R_s^a$ 是不一样的。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.16.PNG" style="zoom:75%;" /><h2 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h2><p>策略$\pi$是一个概率分布或集合，其元素$\pi(a|s)$代表<strong>在给定状态$s$下采取可能action的可能性</strong>。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.17.PNG" style="zoom:75%;" /><ul><li>一个策略完整定义了个体的行为方式，也就是说定义了个体在各个状态下，所采取的可能行为方式及其概率</li><li>在具有Markov性质的决策中，概率分布只取决于当前的状态，与历史无关</li><li>某一确定的Policy是与时间无关，或者说静态的，只和当前面临状态有关。但是个体可以利用算法更新Policy</li></ul><blockquote><p>注意策略是静态的、关于整体的概念，不随状态改变而改变。变化的是在某一个状态时，依据策略可能产生的具体行为。因为具体的行为是有一定的概率的，策略就是用来描述各个不同状态下执行各个不同行为的概率。</p></blockquote><h2 id="理解MP、MRP、MDP的联系"><a href="#理解MP、MRP、MDP的联系" class="headerlink" title="理解MP、MRP、MDP的联系"></a>理解MP、MRP、MDP的联系</h2><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.18.PNG" style="zoom:75%;" /><ul><li>给定一个MDP和策略$\pi$</li><li>MDP中的状态序列可以构成一个马尔可夫过程</li><li>状态和奖励序列组成一个马尔可夫奖励过程</li><li>在这个过程中满足两个方程：<ol><li>在执行某个策略下，状态$s$转移到$s’$发生的概率，等于执行某一个行为的概率与该行为能使状态从$s$转移至$s’$的概率的乘积之和</li><li>同理，当前状态$s$下执行某一指定策略得到的即时奖励，是该策略下所有可能行为得到的奖励与该行为发生的概率的乘积之和</li></ol></li></ul><h2 id="MDP下的两种Value-Function"><a href="#MDP下的两种Value-Function" class="headerlink" title="MDP下的两种Value Function"></a>MDP下的两种Value Function</h2><p>在MDP中, 价值函数可以用来描述针对状态的价值，也可以描述某一状态下执行某一动作的价值。对应<strong>状态价值函数和动作价值价值函数</strong>(严格意义上应该叫状态-动作价值函数的简写)。</p><p>需要注意的是这两种<strong>价值函数的定义都是建立在确定的策略$\pi$上</strong></p><ol><li><p>**基于策略$\pi$的状态价值函数 $v_\pi(s)$**：表示从状态s开始，遵循策略时所获得的期望收益；反映在执行当前策略$\pi$时，个体处于状态s时的价值大小</p></li><li><p>**基于策略$\pi$的动作价值函数 $q_\pi(s,a)$**：遵循策略在状态$s$下执行某一具体动作a所获得的期望收益；或者说在遵循策略$\pi$时，衡量对当前状态执行行为a的价值大小。行为价值函数一般都是与某一特定的状态相对应的。</p></li></ol><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.19.PNG" style="zoom:75%;" /><blockquote><p>这里需要注意的是关于状态$s$和动作$a$的对应关系，在状态价值函数中后续所有可能产生的$G_t$，受到所采取的策略$\pi$的限制（确定性策略的话期望符号其实可以取消，随机性策略仍然是做了期望加权）；而对于动作价值函数，应该理解为在当前状态$s$下强制执行动作$a$（可能依据当前的策略$\pi$并不会采取该动作，当然后续的状态转移是受到策略影响的）。</p></blockquote><p><strong>MDP两种价值函数的关系</strong></p><p>图中空心较大圆圈表示状态，黑色实心小圆表示的是动作本身。可以看出，在遵循策略$\pi$时，状态s的价值体现为在该状态下，遵循某一策略而采取所有可能行为的价值按行为发生概率的乘积求和</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.21.PNG" style="zoom: 67%;" /><p>类似的，一个行为价值函数也可以表示成状态价值函数的形式。它表明某一个状态s下采取一个行为a的价值可以分为两部分：其一是离开这个状态的立即奖励，其二是所有进入新的状态的价值与其转移概率乘积的和。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.22.PNG" style="zoom:67%;" /><h2 id="Bellman-Expectation-Equation"><a href="#Bellman-Expectation-Equation" class="headerlink" title="Bellman Expectation Equation"></a>Bellman Expectation Equation</h2><p>上述状态价值函数和动作价值函数之间关系的两个式子结合起来，就能得到MDP下的贝尔曼期望方程（Bellman Expectation Equation）</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.23.PNG" style="zoom:67%;" /><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.24.PNG" style="zoom:67%;" /><h2 id="MDP的最优求解"><a href="#MDP的最优求解" class="headerlink" title="MDP的最优求解"></a>MDP的最优求解</h2><p>解决强化学习问题意味着要寻找一个最优的策略让个体在与环境交互过程中获得始终比其它策略都要多的收获，一旦找到这个最优策略我们就解决了这个强化学习问题。</p><p><strong>Optimal Value Function</strong></p><p>首先需要对<strong>最优状态价值函数和最优行为价值函数</strong>给出定义：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.25.PNG" style="zoom: 80%;" /><p>可见最优状态价值函数/最优动作价值函数是所有策略下产生的众多状态价值函数中的最大者</p><p><strong>Optimal Policy</strong></p><ul><li>首先需要定义什么是最优策略;</li></ul><p>当对于任何状态$s$，遵循策略$\pi$的价值不小于遵循策略$\pi’$下的价值，则策略$\pi$优于策略$\pi’$：</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.26.PNG"></p><p>对于任何MDP，下面几点成立：</p><ol><li>存在一个最优策略，比任何其他策略更好或至少相等</li><li>可能不止一个最优策略，如果有多个最优策略存在，那么对于任意的状态$s$，所有的最优策略有相同的最优价值函数值$v_*(s)$</li><li>与2同理，所有的最优策略具有相同的行为价值函数$q_*(s,a)$</li></ol><ul><li>寻找最优策略</li></ul><p>MDP的最优策略可以根据最优行为价值函数$q_*(s,a)$来确定，我们之前说到Bellman Expectation Equation中的策略都是关于某一状态的动作概率分布，而对于最优策略则是在某一状态下采取能获得最大action-value的动作（即argmax操作，使得原本的随机动作变成确定性动作）</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.27.PNG" style="zoom:80%;" /><p><strong>Bellman Optimality Equation</strong></p><p>当取得最优策略时，在上面提到的4个Bellman Expectation Equation需要进行改动，得到下面Bellman Optimality Equation（把里面的求状态期望改成取argmax的操作）</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course2.28.PNG" style="zoom:80%;" /><p>虽然MDP可以直接用方程组来直接求解简单的问题，但是更复杂的问题却没有办法求解，因此我们还需要寻找其他有效的求解强化学习的方法。下一篇讨论用动态规划的方法来求解强化学习的问题。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Reinforcement Learning</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>David Silver 强化学习教程（1）：强化学习简介</title>
    <link href="/2020/05/07/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/"/>
    <url>/2020/05/07/David%20Silver%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>人工智能领域无疑是近些年各个行业争相追捧的热点，我一个原本搞通信的现在也在逐渐学习AI的知识，毕竟现在通信的理论基础已经发展遇到瓶颈了，除非再出现一个祖师爷香农级别的人物引领一波理论革命。上一阶段的人工智能，按照个人理解可以概括建立在大数据基础上，让机器从带有标签的大量数据中提取特征，学习模式来完成预测和分类任务。最常见应用就包括计算机视觉、自然语言处理，这两个领域机器已经能做到达到甚至超越人类的性能，而无论公司和从业者都多到离谱，无疑已是一片红海。</p><p>另一类人工智能的分支，以强化学习为代表（当然这里头也涉及到多种技术的融合）。监督式的学习毕竟以人类标记作为准测，机器的性能天花板不可能超过人类。而强化学习才是真正能赋予了机器认知和意识的技术，实现真正意义上的“强人工智能”。以谷歌DeepMind团队开发的AlphaGo系统在2016年和2017年先后击败世界围棋高手李世石和柯洁为标志事件，强化学习算法引起了各个领域的广泛关注，并且取得了不少的研究成果。</p><p>对于各行业从业者，学会强化学习的知识并且用于自己的研究领域无疑是一个迫切的需求。而早在2015年作为领导AlphaGo项目的David Silver就在 UCL 开设了课程并把视频发布到了YouTube上，较为系统地介绍了强化学习的各种思想、实现算法。本系列文章是对于David 所授课程的学习笔记，力求尽量还原教学内容并穿插自己的理解。b站有配上中文字幕得教学视频。<a href="https://www.bilibili.com/video/BV1kb411i7KG?from=search&seid=4606460319322526728">b站链接</a></p><p>本笔记的撰写也参考了以下的文章：</p><ul><li><a href="https://zhuanlan.zhihu.com/reinforce">David Silver 强化学习公开课中文讲解及实践</a></li><li><a href="https://zhuanlan.zhihu.com/p/50478310">David Silver 增强学习——笔记合集</a></li><li><a href="https://www.cnblogs.com/pinard/p/9385570.html">强化学习知识整理</a></li></ul><p>关于参考书籍，David在本课程建议的参考书籍主要有两本：</p><ul><li><a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">《An Introduction to Reinforcement Learning》</a>, （被誉为强化学习教父的Richard S. Sutton编写的经典书籍，也是David Silver的老师，这门课程基本按照这本书的逻辑来），这本书有人翻译成了中文：<a href="https://rl.qiwihui.com/zh_CN/latest/index.html">https://rl.qiwihui.com/zh_CN/latest/index.html</a></li><li><a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">《Algorithms for Reinforcement Learning》</a>，（全书 200+页，较为精简，重视数学逻辑和严格推导，适合喜欢啃公式的同学）</li></ul><h1 id="强化学习简介"><a href="#强化学习简介" class="headerlink" title="强化学习简介"></a>强化学习简介</h1><p><strong>强化学习框架</strong></p><p>下图简单阐述了强化学习的基本框架：智能体（Agent，即图中的大脑）需要通过和环境交互学习解决某项任务。首先智能体通过观察环境（observation）并采取一个动作（action），动作会对环境产生影响从而使环境发生改变，同时环境也会对行为进行反馈一个即时奖励（reward）。智能体重复该过程与环境不断地交互得到很多数据记录，并根据设定的学习算法使用数据改善自身的动作策略。在经过若干次学习后，智能体能最终学会完成相应任务的最优策略（policy）。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.1.JPG" style="zoom:67%;" /></div><p><strong>强化学习与其他机器学习的联系</strong></p><p>如果你对常见的机器学习方法（machine learning）有所涉猎，根据上面对强化学习的描述，可以看出相比于其他的机器学习算法，强化学习具备以下的特点：</p><ul><li>不存在监督者，而是通过和环境交互来获得奖励</li><li>反馈可能存在延迟，当前步骤采取动作带来的效果并不会立即见效（所以一般在实际操作中会设定一个强化学习过程的持续时间）</li><li>数据非独立同分布的：我们得到的决策过程，我们获取的每一次数据都不是独立同分布(i.i.d)的，他是一个有先后顺序的数据序列</li><li>动作对之后的结果能够产生影响：在决策过程中存在先后顺序，当前动作的不同导致下一次得到完全不同的数据</li></ul><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.2.PNG" style="zoom:67%;" /><p>下面这张图说明了强化学习和其他机器学习方法之间的关系：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.15.PNG" style="zoom:67%;" /><h1 id="强化学习中的基本概念"><a href="#强化学习中的基本概念" class="headerlink" title="强化学习中的基本概念"></a>强化学习中的基本概念</h1><p>强化学习入门困难的一点在于其术语非常多且容易混淆，有必要对这些基本概念做详细解释</p><h2 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h2><p>奖励是环境对智能体采取动作做出的反馈，是一个<strong>标量</strong>；它反映了每一步决策的好坏情况；智能体的目标就是最大化累积奖励。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.3.JPG" style="zoom: 80%;" /><p>强化学习本质上是一个<strong>序贯决策过程</strong>，对于任何一个强化学习任务，需要进行以下的考量</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.4.PNG" style="zoom:75%;" /><h2 id="Agent-and-Environment"><a href="#Agent-and-Environment" class="headerlink" title="Agent and Environment"></a>Agent and Environment</h2><p>智能体和环境的交互过程如下：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.5.PNG" style="zoom:67%;" /><ul><li>agent观测环境（observation）并获得环境对其以往动作的反馈（reward），以此为输入，根据内置的算法进而采取对应的行动（action）</li><li>observation是外部环境产生的，可被agent观察到的情况，<strong>不一定等同于environment的内部运行机制</strong>，实际上很多情况下我们也不需要详细知道环境是怎么运作的</li><li>每一个action产生之后会对environment产生作用，影响到下一步骤观测的observation，并且反馈agent相应的reward</li></ul><h2 id="History-and-State"><a href="#History-and-State" class="headerlink" title="History and State"></a>History and State</h2><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.6.PNG" style="zoom: 80%;" /><ul><li>历史是截至时间t为止所能观察到的变量信息，是一个包含过去观测、动作、奖励的序列</li><li>状态是用来规划将来的已有信息，可以看作<strong>是对历史的总结，因此可以被视作关于history的函数</strong>。State相对于history而言最大的好处在于他的信息量很少，实际上，我们也不需要根据所有的历史信息来决策下一步该采取怎样的action（在之后的马尔科夫状态中会提到历史信息的无记忆性）</li></ul><p><strong>3种不同的State</strong></p><ol><li><p><strong>Environment State</strong> $S_t^e$：是真正的环境所包含的信息，一般对agent并不完全可见。实际上，个体有时候也不需要知道环境状态的全部细节，一是因为agent是根据环境反馈的观测/奖励来修改自己的策略；二是即使环境状态对个体完全可见的，这些信息中也可能包含着一些无关成分；</p></li><li><p><strong>Agent State</strong> $S_t^a$：包括个体可以使用的，用来决定未来动作的所有信息。我个人理解是Agent自己对Environment State的解读与翻译，它可能不完整，但我们的确是指望着这些信息来运行强化学习算法的决策</p></li><li><p><strong>Information State</strong>：又称Markov状态，这个概念更多是强调状态的某种性质，与前面的两种State并不形成并列关系。如果现在的状态已经包含了预测未来所有的有用的信息，换句话讲可以丢弃掉历史信息中跟决策影响无关的成分。则它具有马尔科夫性，满足如下定义：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.7.PNG" style="zoom:75%;" /></li></ol><h2 id="Observation-and-Environment"><a href="#Observation-and-Environment" class="headerlink" title="Observation and Environment"></a>Observation and Environment</h2><p>环境可以分为两种：</p><p><strong>1. Fully Observable Environments</strong></p><p>完全可观察环境下，agent能够直接观察到environment state，即满足图中的等式关系。这是一个很理想化的情况，现实中很多复杂问题是不具备这个条件的。同时根据定义，此时的环境是一个<strong>MDP问题：Markov decision process</strong>。也是强化学习最核心的问题。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.8.PNG" style="zoom:75%;" /><p><strong>2. Partially Observable Environments</strong></p><p>部分可观察环境中，Observation state 不等于 environment state，我们只能看到部分信息，或者只能看到一些现象，这也是大多数强化学习问题的场景。这类问题被称为<strong>POMDP问题：partially observable Markov decision process</strong>。所以此时想要解决问题的话Agent必须自己对环境进行解读，自己去探索。对于这类问题David指出一般有以下的解决办法：</p><ul><li>记住所有的历史状态</li><li>使用贝叶斯概率，推导出产生当前环境状态的概率，为此我们需要记录所有环境状态的概率值</li><li>使用递归神经网络，将最近的状态和观测进行组合来推演</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.9.PNG"></p><h2 id="Agent的组成要素"><a href="#Agent的组成要素" class="headerlink" title="Agent的组成要素"></a>Agent的组成要素</h2><p>Agent涉及到三个组成要素：策略（Policy），价值函数（Value Function）和模型（Model），但要注意这三要素不一定要同时具备。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.10.PNG"></p><p><strong>1. 策略 Policy</strong></p><p>策略是决定个体行为的机制。是<strong>从状态到动作的一个映射</strong>。有具体两种表现形式：</p><ul><li><p>确定性的（Deterministic）：在某一特定状态确定对应着某一个行为</p></li><li><p>随机的（Stochastic）：在某一状态下，对应不同行动有不同的概率。随机性的行为更具鲁棒性，因为在一些对抗类的场景下（下围棋），如果agent采取确定性的策略，很容易被对手猜到并作出对付方案</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.11.PNG"></p><p><strong>2. 价值函数 Value Function</strong></p><ul><li>agent在学习过程中需要对面临许多不同的状态，因此要预测某个状态下（或在该状态下状态采取某一动作）可能获得未来的reward的期望值。这个期望值是一个关于状态的函数State-Value function（或关于状态和动作的函数：Action-Value function，在之后笔记的第二章会更详细解释）</li><li>价值函数<strong>是建立在采取某一个策略基础上的</strong>，在不同的策略下即便处于同一状态的，所获得价值并不相同</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.12.PNG"></p><p><strong>模型 Model</strong></p><p>个体对环境的一个建模，相当于agent“脑补”的一个环境。它体现了<strong>个体是如何思考环境运行机制</strong>的（how the agent think what the environment was.），个体希望模型能模拟环境与个体的交互机制。</p><p>模型至少要解决两个问题：一是<strong>预测状态转移概率</strong>：另一项工作是<strong>预测环境的反馈（即时奖励）</strong>：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.13.PNG" style="zoom:75%;" /><p>*<em>[注]**：</em>模型并不是构建agent所必需的，很多强化学习算法并不试图（依赖）构建一个模型。模型仅针对agent的组成而言，环境实际运行机制称为*<em>环境动力学(dynamics of environment)**，它唯一明确agent下一个状态和所得的即时奖励。</em></p><p><strong>Agent的分类</strong></p><ol><li>仅基于价值函数的 <strong>Value Based</strong>：在这样的个体中，有对状态的价值估计函数，但是没有直接的策略函数，策略函数由价值函数间接得到。</li><li>仅直接基于策略的 <strong>Policy Based</strong>：这样的个体中行为直接由策略函数产生，个体并不维护一个对各状态价值的估计函数。</li><li>演员-评判家形式 <strong>Actor-Critic</strong>：个体既有价值函数、也有策略函数。两者相互结合解决问题。</li></ol><p>此外，根据个体在解决强化学习问题时是否建立一个对环境动力学的模型，将其分为两大类：</p><ol><li><strong>Model free</strong>: 这类个体并不视图了解环境如何工作，而仅聚焦于价值和/或策略函数。</li><li><strong>Model based</strong>：个体尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。</li></ol><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david_course1.14.PNG" style="zoom:75%;" /><h1 id="强化学习算法中的几组理念"><a href="#强化学习算法中的几组理念" class="headerlink" title="强化学习算法中的几组理念"></a>强化学习算法中的几组理念</h1><h2 id="Learning-amp-Planning"><a href="#Learning-amp-Planning" class="headerlink" title="Learning &amp; Planning"></a>Learning &amp; Planning</h2><ul><li>学习：<strong>环境初始时是未知的</strong>，个体不知道环境的信息，只能通过和环境来互动得知我们的action会造成什么样的改变，逐渐改善其行为策略。</li><li>规划: <strong>环境如何工作对于个体是已知或近似已知的</strong>，个体并不与环境发生实际的交互，而是利用其构建的模型进行计算，在此基础上改善其行为策略。Planning 问题可以使用动态规划来解决，在本系列的第三部分我们会具体谈到这个方法。</li></ul><p>一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。</p><h2 id="Exploration-amp-Exploitation"><a href="#Exploration-amp-Exploitation" class="headerlink" title="Exploration &amp; Exploitation"></a>Exploration &amp; Exploitation</h2><p>强化学习是一个不断试错，然后减少错误率的过程。所以这里存在一个矛盾，当我们存在一个相对比较好的解决方案时，我们应该选择沿用我们的解决方案，还是选择尝试新的未知的路径，这个路径可能有很高的错误率，也可能有更好的解决方案。如何进行两者的平衡，就是Exploration和Exploitation的问题</p><ul><li>Exploration(探索)：倾向于探索环境中新的信息</li><li>Exploitation(利用)：倾向于利用已知的信息来获取最大reward</li></ul><h2 id="Prediction-amp-Control"><a href="#Prediction-amp-Control" class="headerlink" title="Prediction &amp; Control"></a>Prediction &amp; Control</h2><p>在强化学习里，我们经常需要先解决关于预测（prediction）的问题，而后在此基础上解决关于控制（control）的问题。实际上，这两者是递进的关系。</p><ul><li><p>prediction：<strong>给定一个policy</strong>，评价遵循该策略下agent能获得多大的奖励，可以看成是求解在给定策略下的价值函数（value function）的过程。</p></li><li><p>control：<strong>在没有policy的前提下</strong>，直接找出一个好的策略来最大化未来的奖励。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Reinforcement Learning</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>理解信道衰落、相干带宽、相干时间</title>
    <link href="/2020/02/24/%E6%97%A0%E7%BA%BF%E4%BF%A1%E9%81%93%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"/>
    <url>/2020/02/24/%E6%97%A0%E7%BA%BF%E4%BF%A1%E9%81%93%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="无线信道衰落的分类"><a href="#无线信道衰落的分类" class="headerlink" title="无线信道衰落的分类"></a>无线信道衰落的分类</h1><p>无线通信相比有线通信，存在着两个显著的特点使得其问题更为复杂。</p><ol><li>无线信道的状态是随时间不断变化的</li><li>无线通信的环境是开放的，所有用户传输的信号都处在同一个环境下，会造成对别的用户的干扰</li></ol><p>电磁波作为传输载体，存在着多种的传播方式：直射、散射、反射、绕射等，这些都会对信号的强度造成影响。无线信号的衰减方式主要可分为：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/wireless fading.png" style="zoom:80%;" /><ul><li>大尺度衰落：<ul><li>自然衰减（路径损耗）：一般可认为信号功率与传输距离呈现$1/r^2$关系衰减（自由空间或者近距离），或者与距离呈指数递减关系：$r^{\alpha}$（实际的传输环境）</li><li>阴影衰落：信号传输中遇到起伏的地形，建筑物等，因为阻塞而造成的衰减</li></ul></li><li>小尺度衰落：<ul><li>多径效应：传输环境中存在多条通信路径，各条发射波到达接收机的时间，或者说相位各不相同，在接收点叠加造成剧烈的变化</li><li>多普勒效应：由于移动导致接受信号频率的偏移，向发射端移动时频率会增加，而远离时会减小，且移动速度越高，所产生的效应越大</li></ul></li></ul><p>大尺度、小尺度是针对移动程度来说的。一般来讲，小尺度在很短的移动距离内（可认为与信号的波长相当），而大尺度衰落随移动变化较为缓慢，一般通过中继放大器、直放站、室内分布系统等来补偿衰落。</p><p>![](<a href="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/various">https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/various</a> fading.JPG)</p><h1 id="时延扩展与相干带宽"><a href="#时延扩展与相干带宽" class="headerlink" title="时延扩展与相干带宽"></a>时延扩展与相干带宽</h1><p>由于多径传输的原因，假如在发送端发送一个窄的脉冲信号，在接收端会接收多个脉冲，本来最短时延是沿最短路径传输所消耗的时间，现在因为多条路径长短不一，所以时延被扩展了，这也称为信道的**时间弥散性（time dispersion)**。通常将最后一个到达的脉冲，和最先到达的脉冲的时延差，称为最大时延扩展，用$\tau_{max}$来表示。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/time spread.png" style="zoom:67%;" /><ul><li><p>从时域上看，如果这个时延扩展大于两个发送脉冲之间的时间间隔，则会对下一个脉冲的接收产生干扰。，造成符号间干扰即码间串扰 **Inter Symbol Interference，ISI)**，</p><p>要避免这种干扰，就要将发送符号之间的周期$T$扩大。扩大多少呢？至少是大于最大时延扩展: $T_{min}&gt;\tau_{max}$，这样多径时延的影响就很小，没有符号间干扰。根据周期信号的时域频域关系，应该存在一个最大的频带宽度$f_{c}=1/T_{min}$，称为为<strong>相干带宽</strong>，它约等于多径时延的倒数。</p></li><li><p>从频域上看，即<strong>信号带宽或者信号发送速率远大于信道的相干带宽时，</strong>信号通过无线信道后某些频率成分信号的幅值可以增强（相位相同，幅值叠加），而另外一些频率成分信号的幅值会被削弱（相位相反，幅值相消），引起信号波形的失真，此时就认为发生了<strong>频率选择性衰落。</strong> 当信号满足小于相干带宽的要求，则认为接收端的信号通过无线信道后，各频率成分幅度很接近（称为“相干”），即经历了<strong>平坦衰落</strong></p><p>频率选择性衰落，实质是一样的，在时域中是符号间干扰，表现形式是信号波形会发生畸变，换算到频域中来看，就是有些频率分量的强度会加强，有些会减弱。<strong>相干带宽是无线信道的一个特性</strong>， 至于信号通过无线信道时， 是出现频率选择性衰落还是平坦衰落， 这要取决于信号本身的带宽与相干带宽的大小关系。</p></li><li><p>应用：对于高速通信（很高的信号带宽），为了对抗频率选择性衰落，人们采用了正交频分复用（OFDM）技术，将宽带信号分成很多子带，频域上分成很多子载波发送出去，每个子带的信号带宽由于小于相干带宽，从而减少甚至避免了频率选择性衰落。</p></li></ul><h1 id="多普勒扩展与相干时间"><a href="#多普勒扩展与相干时间" class="headerlink" title="多普勒扩展与相干时间"></a>多普勒扩展与相干时间</h1><p>多普勒频移的计算公式如下，其 中$f_c$表示载波频率， $c$表示光速，$f_m$ 表示最大多普勒频移， $v$表示移动台的运动速度，$\theta$表示信号到达角（物体前进方向与信号到达方向的夹角）。</p><p>![](<a href="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/doppler">https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/doppler</a> shift.png)</p><p>由于多普勒频移，导致了一频率信号经过时变衰落信道之后会呈现为具有一定带宽和频率包络的信号，见下图。这又可以称为信道的<strong>频率弥散性(frequency dispersion)</strong></p><p>![](<a href="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/frequence">https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/frequence</a> dispersion.png)</p><p><strong>相干时间</strong>被定义为：$T_c \approx1/f_d$，等于多普勒频移的倒数。它是指<strong>信道状态维持不变的时间</strong>，或者说信道的冲激响应维持不变的时间间隔。</p><ul><li>从时域上看，若<strong>符号的持续时长</strong>远大于信道的相干时间，多普勒扩展带宽内所对应信号的相位将发生很大的变化，导致在一个符号时间内接收端到达的同一信号的波形产生很大的变化，产生<strong>时间选择性衰落，也称为快衰落</strong>；否则，在此时间内接收信号的包络的相位变化将很小，信号的幅度变化也就很小，产生<strong>非时间选择性衰落，也称为慢衰落。</strong></li><li>从频域上看，若信号带宽远小于多普勒扩展带宽，那由于频移使得相邻的频率分量之间相互干扰，造成<strong>载波间干扰（ICI）。</strong></li></ul><p>参考David Tse的《无线通信基础》：多普勒扩展对应的频率就是信号变化的包络，这个包络的周期即相干时间正比于就是多普勒扩展带宽的倒数。</p><p>![](<a href="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david">https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/david</a> tse.png)</p><ul><li>应用：从发射分集的角度来理解：时间分集要求两次发射的时间要大于信道的相干时间，即如果发射时间小于信道的相干时间，则两次发射的信号衰落特性完全相似，会被接收端认为是同一个信号。分集抗衰落的作用就不存在了。</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>无线通信</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读01]Neurosurgeon：Collaborative Intelligence Between the Cloud and Mobile Edge</title>
    <link href="/2019/12/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%8ANeurosurgeon%EF%BC%9ACollaborative%20Intelligence%20Between%20the%20Cloud%20and%20Mobile%20Edge%E3%80%8B/"/>
    <url>/2019/12/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%8ANeurosurgeon%EF%BC%9ACollaborative%20Intelligence%20Between%20the%20Cloud%20and%20Mobile%20Edge%E3%80%8B/</url>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://web.eecs.umich.edu/~jahausw/publications/kang2017neurosurgeon.pdf">《Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge》</a></p><p><strong>Neurosurgeon：一种云和移动边缘之间的联合智能系统</strong></p><p>论文出处：2017 <strong>ASPLO</strong> (Architectural Support for Programming Languages and Operating Systems)，是计算机体系结构的顶级学术会议。</p></blockquote><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>目前越来越多的智能应用程序，使用语音和图像类型的数据作为输入。比如智能个人助理（Apple Siri、Microsoft Cortana），智能家居和可穿戴设备、导航应用等，人们与移动设备交互模式正在发生迅速改变，预计将取代传统的基于文本的输入形式。</p><p>这些应用的实现依赖准确且高度复杂的机器学习技术，其中最常见的是深度神经网络（DNN）。早期的工作认为：传统的移动设备不能支持这种大量的计算，以满足合理的延迟和能量消耗。Web服务提供商用于智能应用程序的现行方法，是把用户的移动设备生成的查询发送到云进行处理，在远端的高性能云服务器上托管所有计算。然而利用这种方法，大量数据（例如，图像，视频和音频）经由无线网络和回程链路上载到服务器，导致高等待时间和能量成本。</p><p>庆幸的是，现代移动硬件的性能和能效通过强大的移动SoC集成继续得到改善。受此启发，本文的作者重新审视了移动和云之间智能应用程序的计算细分。即在运行以神经网络为支撑的应用程序任务时，将计算任务拆解为server-side和edge-side两部分，其中server-side在数据中心的服务器里执行，edge-side则在终端设备上执行。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/1.PNG"></p><p>这项工作要处理的主要问题包括：</p><ol><li>在当今的移动平台上执行大规模智能工作负载的可行性如何？</li><li>在何种情况下，通过无线网络传输语音和图像数据的成本过高，而无法发挥云处理的合理性？</li><li>在为需要大量计算的智能应用程序提供处理支持方面，移动边缘应当扮演什么角色？</li></ol><p>作者使用8个基于DNN的智能应用程序进行试验，这些应用程序涵盖了视觉，语音和自然语言领域。作者发现：终端设备与数据中心的网络连接带宽，设备的计算能力，DNN模型的结构都会影响到计算任务的拆解方式。拆解的粒度是神经网络的layer。这种任务划分策略的考虑，可以影响端到端延迟和移动能源效率。这也可以被认为是一个带约束的优化问题，优化目标是对计算任务在两个计算结点（终端和服务器端）之间进行拆分，约束则包括网络结构、网络带宽以及设备的计算能力。同时，将计算推到云之外的移动设备上，也提高了数据中心的吞吐量，允许给定的数据中心支持更多的用户查询，为移动设备和云系统创造了双赢的局面。</p><p>作者为此设计了一个轻量级动态调度系统：Neurosurgeon。它是一个跨越云和移动平台的系统，可自动识别DNN中的理想分区点，并协调移动设备和数据中心之间的计算分配。针对8个DNN应用程序的评估表明，使用Neurosurgeon，平均可将端到端延迟提高3.1倍，将移动能耗降低59.5％，并将数据中心吞吐量提高1.5倍。</p><h1 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h1><p>本部分，主要是对深度神经网络（DNN）的概述，作者描述计算机视觉，语音和自然语言处理等应用程序，如何利用DNN作为其核心机器学习算法。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/2.PNG"></p><h1 id="3-Cloud-only-Processing"><a href="#3-Cloud-only-Processing" class="headerlink" title="3. Cloud-only Processing"></a>3. Cloud-only Processing</h1><h2 id="Experimental-setup"><a href="#Experimental-setup" class="headerlink" title="Experimental setup"></a>Experimental setup</h2><p>介绍了本文实验中，移动端和服务器端采用的硬件平台：</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/3.PNG"></p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/4.PNG"></p><p>以及采用的DNN网络的搭建框架：</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/5.PNG" alt="5"></p><h2 id="Examining-the-Mobile-Edge"><a href="#Examining-the-Mobile-Edge" class="headerlink" title="Examining the Mobile Edge"></a>Examining the Mobile Edge</h2><p>本文作者以经典的AlexNet网络（一个典型的用于图像分类的卷积神经网络）作为示例，网络以152KB图像作为输入。在通信延迟、计算延迟、端到端延迟和能耗方面进行对比。对于无线通信，使用TestMyNet软件在多个移动设备上测量3G，LTE和Wi-Fi的带宽。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/6.PNG"></p><p>先看时延这一指标：</p><ul><li><strong>通信延迟</strong> ：图3a显示了通过3G，LTE和Wi-Fi上传输入图像的延迟。表明网络类型对于实现将数据传送至云端的低延迟至关重要。</li><li><strong>计算延迟</strong>：图3b显示了在移动CPU，GPU和云GPU上运行AlexNet网络的计算延迟。可以看出在移动设备上运行神经网络的性能还是和云端服务器存在差距；移动设备上GPU处理要强过CPU处理；注意到，即便移动CPUs上运行AlexNet处理图像的时间，仍然比通过3G上传输数据输入快2.3倍。</li><li><strong>端到端延迟</strong> ：图3c显示了仅在云端处理和仅在移动设备上处理，两种方式所需的总延迟，每个条形顶部的注释是用于计算所花费的端到端延迟的占比。<ul><li>使用云服务器进行全部的计算时，结果表明计算所消耗的时间仅占全部时间的6%，而剩余的94%都消耗在数据传输上。</li><li>只要移动设备拥有可用的GPU，在本地GPU上实施所有的计算操作能够带来最佳的体验（总延迟时间最短）；同时，在LTE和Wi-Fi网络条件下，传输至云端处理要比仅用移动设备CPU进行全部的计算操作要更好（系统延迟时间更少）。</li></ul></li><li><strong>能耗</strong>：如果移动设备连接的是Wi-Fi网络，最低的电量损耗方案是发送相应的数据到云服务器并让其进行全部的计算操作。但如果连接的是3G或LTE网络，并且该移动设备有可用的GPU，那么在本地GPU上实施全部的计算操作这一方案所导致的电量消耗，会比数据传输且在云服务器上实施全部的计算操作这一方案更低。</li></ul><h1 id="4-Fine-grained-Computation-Partitioning-细粒度计算分区"><a href="#4-Fine-grained-Computation-Partitioning-细粒度计算分区" class="headerlink" title="4. Fine-grained Computation Partitioning 细粒度计算分区"></a>4. Fine-grained Computation Partitioning 细粒度计算分区</h1><p>上一章所有计算都实施在云服务器或移动设备上，这两种“相对极端”的方法之间，或者说在数据传输和实施计算两者之间是否存在一种折中。显然，DNN分层级的网络结构，提供适合于分区计算的抽象概念。</p><h2 id="Layer-Taxonomy"><a href="#Layer-Taxonomy" class="headerlink" title="Layer Taxonomy"></a>Layer Taxonomy</h2><p>这一小节主要是对目前DNN中存在的各种类型的层和其功能的简要介绍，涉及深度学习有关的知识。包括：Fully-connected Layer、Convolution &amp; Local Layer、Pooling Layer、Activation Layer、normalization layer、softmax layer、argmax layer、dropout layer等。</p><h2 id="Characterizing-Layers-in-AlexNet"><a href="#Characterizing-Layers-in-AlexNet" class="headerlink" title="Characterizing Layers in AlexNet"></a>Characterizing Layers in AlexNet</h2><p>首先研究AlexNet中每个层的数据和计算特征（在本节及后续部分中，在移动和服务器平台中均使用GPU），将得到如下的统计图。（浅色柱）显示了移动GPU上按顺序执行AlexNet每个层的延迟，（深色柱）显示了每层输出数据的大小，这也是下一层的输入。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/7.PNG"></p><ul><li><p>每一层都有不同的计算时间和数据产生开销。</p></li><li><p>卷积（conv）和全连接层（fc）是最耗时的层，占总执行时间的90％以上。</p></li><li><p>中间的卷积层（conv3和conv4）比早期的卷积层（conv1和conv2）花费更长的时间。这是因为后面的卷积层会使用更多数量的滤波器以提取更强的特征，从而增加计算量。</p></li><li><p>AlexNet的前几层会生成大量输出数据，而池化层（pool）会将数据数量锐减，数据数量通过激活层保持不变（relu1-relu5），最后的几层（位于深层的全连接层，softmax，argmax）会逐渐减小数据大小直到最后将数据减少为一个分类标签。</p></li></ul><p>对于AlexNet这种结构的DNN来说，位于网络前端的层产生较大的数据量，而后端的层则带来了较大的计算开销。</p><h2 id="Layer-granularity-Computation-Partitioning-层粒度计算分区"><a href="#Layer-granularity-Computation-Partitioning-层粒度计算分区" class="headerlink" title="Layer-granularity Computation Partitioning 层粒度计算分区"></a>Layer-granularity Computation Partitioning 层粒度计算分区</h2><p>下面研究如果各个网络层分割开会发生什么呢？（即在移动设备上处理模型的前n层，再把得到的第n层的输出结果传输至云服务器上进行之后的计算，最后再将输出结果传输至移动设备上）在本节中，使用Wi-Fi作为无线网络配置。</p><p>在下图中的每个条形，意味着在这一层之后进行分区的端到端延迟和移动能量消耗。那么，最左边的条表示发送原始输入进行云端处理；最右边的条是在移动设备上本地执行整个DNN。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/8.PNG" alt="8"></p><ul><li><strong>延迟分区</strong> ： 如果在前端进行分区，则数据传输主导端到端延迟，这与之前观察到的数据大小在DNN早期阶段的变化趋势是一致的。在后端进行分区可以提供更好的性能，这样可以最大限度地减少数据传输开销，同时后端网络层需要大量的计算，可以利用功能强大的云端服务器执行。在AlexNet使用移动GPU和Wi-Fi的情况下，在pool5和fc6层之间的分区实现了最低延迟，比全部传送到云处理上提高了2.0倍 。</li><li><strong>能量分区</strong> –：与延迟类似，由于无线数据传输的高能源成本，转移仅用于云的处理的输入并不是最节能的方法。 如图6b所示，DNN中间的分区实现了最佳的移动能耗，比纯云方法节能18％。</li></ul><h2 id="Generalizing-to-More-DNNs"><a href="#Generalizing-to-More-DNNs" class="headerlink" title="Generalizing to More DNNs"></a>Generalizing to More DNNs</h2><p>作者进一步将实验扩展到7个更智能的应用程序（7个不同结构的DNN），以研究它们的数据和计算特性及其对计算分区的影响。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/9.PNG" alt="9"></p><p>对于应用于计算机视觉 (Computer Vision, CV) 领域的具有卷积层的模型，由于卷积层后的数据大小增加，通常最佳的分割点在模型的中部；而对于通常只有全连接层和激活层的网络模型（主要应用在语音识别（Automatic Speech Recognition, ASR）和自然语言处理领域(Natural Language Process, NLP)）而言，在模型的开始部分或者结尾部分进行分割往往更好一点。可以看出，最佳分割点随着模型的不同而变化着。</p><p>基于不同DNN结构的应用程序的最佳分区点的变化表明：需要一种系统能够智能地选择划分DNN的最佳点以优化端到端延迟或移动设备能量消耗，并利用云服务器和设备GPU分配相应的计算操作。这也引出了下一章这篇文章的核心工作。</p><h1 id="5-Neurosurgeon"><a href="#5-Neurosurgeon" class="headerlink" title="5. Neurosurgeon"></a>5. Neurosurgeon</h1><p>对于一个DNN模型，影响最佳的分割点位置的因素主要有两种：一种是静态的因素，例如模型的结构；一种是动态的因素，即使对于相同的DNN架构，诸如无线网络状态、数据中心负载和设备剩余可用的电量等的动态因素也会影响最佳分区点。</p><p>基于以上的因素，作者为此提出了智能DNN分区引擎Neurosurgeon的设计，由部署阶段和运行时系统组成。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/10.PNG" alt="10"></p><ul><li><strong>部署阶段</strong>：描述移动设备和服务器，以生成DNN常用的各种层类型的性能预测模型，需要注意的是这和应用程序无关的，只需要针对给定的移动和服务器平台进行一次。这组预测模型存储在移动设备上，随后用于预测每层的延迟和能量成本</li><li><strong>运行阶段</strong> ： 在移动设备上执行基于DNN的智能应用程序期间，Neurosuron会动态确定DNN的最佳分区点。 步骤如下：<ol><li>分析并提取DNN架构的图层类型和配置；</li><li>系统使用已存的层性能预测模型来估计在移动和云上执行每一层的延迟和能量消耗；</li><li>通过这些预测，结合当前的无线连接带宽和数据中心负载水平，Neurosurgeon选择最佳分区点，优化最佳端到端延迟或最佳移动能耗；</li><li>执行DNN，在移动和云之间进行分区工作。</li></ol></li></ul><h1 id="6-Evaluation"><a href="#6-Evaluation" class="headerlink" title="6. Evaluation"></a>6. Evaluation</h1><p>实验结果表明，相比于目前使用仅使用云服务器的方法，Neurosurgeon能够将应用的延迟时间平均降低了3.1倍（最高能达到40.7倍）。</p><p>在电量消耗方面，相比于现有方法，Neurosurgeon能够使得移动设备的电量消耗量平均降低至59.5%，最高能降低至94.7%。</p><p>下图展示了Neurosurgeon随着网络环境的变化（即LTE带宽变化）自适应进行分割和优化的结果（下图中的蓝色实线部分），可以看出比起现有方法（下图中的红色虚线部分）能够大幅度地降低延迟时间。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/11.PNG"></p><p>Neurosurgeon也会与云服务器保持周期性的通讯，以获得其数据中心的负载情况。当服务器的数据中心负载较大时，它会减少往服务器上传输的数据量而增加移动设备本地的计算量。总之，Neurosurgeon能够根据服务器的负载情况作出适当的调整已达到最低的系统延迟时间。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/12.PNG" alt="12"></p><p>随着连接网络质量的变差，Neurosurgeon会让移动设备承担更多的计算量，此时云服务器上数据中心的吞吐量将增加：较现有方法，连接LTE网络的情况下数据中心的吞吐量增加至1.43倍，而3G网络条件下则增加至2.36倍。此外还可以观察到，随着带GPU的移动设备百分比增加，Neurosurgeon增加了从云端到移动设备端的计算，导致更高的数据中心吞吐量改进。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/13.PNG"></p><h1 id="个人体会"><a href="#个人体会" class="headerlink" title="个人体会"></a>个人体会</h1><p>这个文章更多是一个偏实验性质的论文，但有着很好的指导意义。</p><ol><li><p>不少和MEC相关的综述文章，对于可分割计算任务，给出的是如下图这样很抽象的模型描述，这篇文章把计算任务具体到以DNN为核心的有关运算，能帮助理解这些抽象模型，实际上这也是边缘应用中非常普遍的计算任务之一（许多需要边缘计算的领域，需要运行神经网络为基础的应用程序）。</p></li><li><p>作者给出了详细的实验设备、DNN框架、经典DNN模型的指导，应该来讲这篇论文的可复现的程度是很高的</p></li><li><p>作者指出这种DNN为核心的计算任务，应该在移动端和边缘服务器上联合计算。对于计算量的划分，会受到DNN模型结构这一静态因素，和诸如无线网络状态、数据中心负载和设备剩余可用的电量等动态因素影响。对于动态因素在本文里多是以实验仿真去评估，作者实际并没有做很深入的数学上的分析。</p></li><li><p>这类DNN为核心的计算任务，尤其是其中视觉有关的DNN模型。在工业场景下应该会存在更大的应用，如：车间摄像头实时监控流水线的设备运作画面、无人机航拍城市交通等，并通过无线网络上传画面配合边缘服务器联合计算。这类工业场景中的无线网络不能总保持良好状态，会影响计算结果的实时性（论文中指端到端时延，是否可以考虑引入信息年龄AoI的概念？）</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>文献阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>边缘计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《CSAPP》读书笔记（2）：信息的表示和处理</title>
    <link href="/2019/11/12/%E3%80%8ACSAPP%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%EF%BC%9A%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/"/>
    <url>/2019/11/12/%E3%80%8ACSAPP%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%EF%BC%9A%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="信息存储"><a href="#信息存储" class="headerlink" title="信息存储"></a>信息存储</h1><ul><li>现代计算机是建立在对信息的<strong>bit级</strong>表示上的，大多数计算机是使用8位的块，即<strong>字节</strong></li><li>机器级程序将内存视为一个非常大的字节数组，称为<strong>虚拟内存</strong></li><li>内存中的每个字节由一个唯一的数字表示，称为它的<strong>地址</strong>。所有可能地址的集合称为<strong>虚拟地址空间</strong>，当然这只是展现给程序的概念性印象，具体实现见第9章</li></ul><h2 id="16进制表示法"><a href="#16进制表示法" class="headerlink" title="16进制表示法"></a>16进制表示法</h2><p>用0-1bit的串来表示是很麻烦的，因此采用的是16进制表示法。</p><h2 id="字长"><a href="#字长" class="headerlink" title="字长"></a>字长</h2><p>每台计算机都有一个字长，也就是平常所谓的32位、64位机器。它决定着<strong>虚拟地址空间的大小（虚拟内存的字节总数）</strong></p><blockquote><p>我们称呼程序为“32位程序”或“64位程序”，针对该程序是如何编译的，而不是具体运行的机器类型。</p><p>以C语言为例，其不同的数据类型占多少字节的内存空间，是受编译器说了算</p></blockquote><h2 id="寻址和字节顺序"><a href="#寻址和字节顺序" class="headerlink" title="寻址和字节顺序"></a>寻址和字节顺序</h2><p>对于跨越多个字节的对象，需要建立的规则就是：这个对象的地址是什么，以及在内存中如何排列组成对象的这些字节？</p><ul><li>几乎所有计算机都是以该<strong>对象所使用字节中最小的地址</strong>，作为对象的地址。</li><li>接下来对于所有字节，有两种规则的排序：<ul><li><strong>小端法</strong>（little endian）：大多数Intel兼容机采用的方法，对象的最低有效字节放在内存的低地址端</li><li><strong>大端法</strong>（big endian）：IBM 和Oracle的机器，对象的最低有效字节放在内存的低地址端</li></ul></li></ul><h2 id="字符的表示"><a href="#字符的表示" class="headerlink" title="字符的表示"></a>字符的表示</h2><p>对于文本（字符数组）的表示方法，链接中的视频是个很好的说明：</p><blockquote><p><a href="https://www.bilibili.com/video/av23469929?from=search&amp;seid=3003390791592500042">https://www.bilibili.com/video/av23469929?from=search&amp;seid=3003390791592500042</a></p></blockquote><h2 id="布尔代数"><a href="#布尔代数" class="headerlink" title="布尔代数"></a>布尔代数</h2><p>英国数学家乔治$\cdot$布尔所创立的逻辑体系，也是现代计算机的基础之一；香农是第一个把布尔运算和数字电路联系起来的人（1937年的硕士论文）。</p><h2 id="C语言的位级运算和逻辑运算"><a href="#C语言的位级运算和逻辑运算" class="headerlink" title="C语言的位级运算和逻辑运算"></a>C语言的位级运算和逻辑运算</h2><ul><li><code>&amp;</code>，<code>|</code>，<code>~</code>：C语言支持按位布尔运算</li><li><code>&amp;&amp;</code>，<code>||</code>，<code>！</code>：C语言还提供了一组逻辑运算符，很容易和位运算混淆；此外，逻辑运算符支持<strong>短路特性</strong></li></ul><h2 id="C语言的移位运算"><a href="#C语言的移位运算" class="headerlink" title="C语言的移位运算"></a>C语言的移位运算</h2><p>C语言中包括三种移位操作，三者实现如下：</p><ul><li><strong>左移</strong>（&lt;&lt;）：移出去的位丢弃，空缺位（vacant bit）用 0 填充</li><li>右移（&gt;&gt;）<ul><li><strong>逻辑右移</strong>：移出去的位丢弃，空缺位（vacant bit）用 0 填充；</li><li><strong>算术右移</strong>：移出去的位丢弃，空缺位（vacant bit）用符号位来填充。</li></ul></li></ul><p><img src="1.PNG"></p><h1 id="整数表示"><a href="#整数表示" class="headerlink" title="整数表示"></a>整数表示</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《CSAPP》读书笔记（1）：计算机系统漫游</title>
    <link href="/2019/11/11/%E3%80%8ACSAPP%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%BC%AB%E6%B8%B8/"/>
    <url>/2019/11/11/%E3%80%8ACSAPP%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%BC%AB%E6%B8%B8/</url>
    
    <content type="html"><![CDATA[<p>这个系列是对于《Computer Systems: A Programmer’s Perspective》，CSAPP第三版的读书笔记，对应的中文教材译为《深入理解计算机系统》，在b站有对应的教学视频：<a href="https://www.bilibili.com/video/BV1iW411d7hd">2015 CMU 15-213 CSAPP 深入理解计算机系统 课程视频</a></p><p>第一章内容主要针对视频课程中没有涉及的书本第一章，作为对整本书的概述，包括了以下几个计算机系统常涉及到的内容：</p><h1 id="计算机的硬件组成"><a href="#计算机的硬件组成" class="headerlink" title="计算机的硬件组成"></a>计算机的硬件组成</h1><p>为了理解程序运行时发生了什么，我们需要了解一个典型系统的硬件构成。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/2.png" style="zoom:67%;" /><ol><li>**总线(buses)**：通常总线被设计成传送定长的字节块，也就是字。字中的字节数(字长word size)，是一个基本的系统参数，大多数机器为4个字节(32位)或8个字节(64位)</li><li>**I/O设备(I/O devices)**：每个I/O设备都通过一个控制器(controller)或者适配器(adapter)与I/O总线相连接。控制器和适配器的区别主要在于其封装方式：控制器是置于I/O设备本身的或主板上的芯片组，而适配器则是一块插在主板插槽上的卡</li><li>**主存(main memory)**：主存即内存，是一个CPU能直接寻址的临时存储设备，在处理器执行程序时用来存放程序和程序处理的数据。物理上，主存由一组DRAM组成。逻辑上，存储器是一个线性的字节数组，每一个字节都有其唯一的地址(数组索引)，这些地址均从零开始。</li><li>**处理器(processor)**：一个CPU由若干部分组成：<ul><li>寄存器：通常为8位寄存器，用来保存一个字节的数据。CPU中有若干寄存器，每个寄存器都有唯一的地址，用来保存CPU中临时运算结果。其中有两个寄存器比较特殊：</li><li>指令地址寄存器：用来保存当前指令在内存中的地址，每次执行完一条指令后，会对该寄存器的值进行修改，指向下一条指令的地址。</li><li>指令寄存器：用来保存当前从主存中获取的，需要执行的指令。</li><li>ALU：算术逻辑单元，主要用来处理CPU中的数学和逻辑运算。它包含两个二进制输入，以及一个操作码输入，用来决定对两个输入进行的算数逻辑操作。然后会输出对应的运算结果，以及具有各种标志位，比如结果是否为0、结果是否为负数等等。</li><li>控制单元：是一系列门控电路，通过门控电路来判断指令寄存器中保存的指令内容，然后调整控制主存和寄存器的读写数据和地址，以及使用ALU进行运算。我的理解就是一系列门控电路，然后根据你程序的指令来调控CPU中的各种资源。</li></ul></li></ol><p>CPU中执行指令的过程：首先根据指令地址寄存器从内存中获取对应地址的数据，然后将其保存在指令寄存器中，然后控制单元会对指令内容进行判断，并调用寄存器、ALU等执行指令内容，然后更新指令地址寄存器，使其指向下一个要执行的指令地址。</p><h1 id="一个hello-world程序的执行过程"><a href="#一个hello-world程序的执行过程" class="headerlink" title="一个hello world程序的执行过程"></a>一个hello world程序的执行过程</h1><p>一个最简单的hello world C语言程序如下：</p><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;stdio.h&gt;</span></span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;hello world\n&quot;</span>);    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>这段代码需要保存在一个文件中，称为源文件，这是这段程序生命周期的开始。现代计算机只知道0和1的二进制数，为理解这段文本到底是什么意思，计算机系统会使用ASCII标准来表示这些文本，简单来说就是给每个字符都指定一个唯一的单字节大小的编号，然后将文本中的字符都根据ASCII标准替换成对应的编号后，就转换成了字节序列，即<strong>源文件是以字节序列的形式保存在文件中</strong>。对于这样只有ASCII字符构成的文件称为<strong>文本文件</strong>，所有其他文件都称为<strong>二进制文件</strong>。</p><p>使用高级的C语言编写这段程序是为了方便人理解，但是对于计算机来说太过于复杂了，它只能执行指令集中包含的指令。所以为了能够在系统中运行这段程序，我们需要先将每句C语句都转换成一系列的低级机器语言指令，然后这些指令按照可执行目标程序的格式打包好后，以二进制磁盘文件形式保存起来，该文件称为目标文件。这种从源文件转换到目标文件的过程由编译系统(compilation system)驱动的。该过程主要分为4个阶段：</p><p>包括以下几个部分：<strong>预处理器(pre-processor)、编译器(compiler)、汇编器(assembler)、链接器(linker)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/1.png"></p><ul><li>预处理：处理字符<code>#</code>开头的命令，具体包括以下步骤：<ul><li>将头文件的内容插入程序文本中</li><li>宏定义替换</li><li>条件编译(<code>#if</code> <code>#ifdef</code>)，不被编译的部分变为空行</li><li>删除注释</li></ul></li><li>编译：编译器将C语言的<code>hello.i</code>翻译成汇编语言的<code>hello.s</code>。 这样做的好处在于，通过为不同语言不同系统上配置不同的编译器，能够提供通用的汇编语言，这样对于相同的语言，就能兼容不同的操作系统，而对于同一个系统上，通过安装不同语言的编译器，也能运行不同语言写的程序了。</li><li>汇编：将汇编程序翻译成机器语言指令，并将其打包成一种叫可重定位目标程序(relocatable object program)的二进制文件</li><li>链接：链接器将各个.o文件合并成可执行文件。链接器使得分离编译成为可能。在编写大型程序时，通常会使用到各种函数库，但是我们代码中并没有这些函数的具体实现，所以就需要在链接阶段将该函数的具体实现合并得到可执行文件，加载到内存中，由系统执行。</li></ul><p>下面展示了可执行文件从磁盘加载到内存执行，并在设备上显示结果的过程：</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/3.png" style="zoom:67%;" /><h1 id="高速缓存"><a href="#高速缓存" class="headerlink" title="高速缓存"></a>高速缓存</h1><p>问题：如上图的程序执行过程所示，hello程序最初存放在磁盘上，当程序加载时，被复制到内存中，当CPU运行时，指令又被复制到CPU中。这些过程会花费大量时间将代码和数据进行复制，减缓了程序“真正”的工作，如果使这些复制尽快完成就能进行系统加速。</p><ul><li>首先根据机械原理可知，较大的存储设备比较小的存储设备运行得慢，而高速设备的造价远高于同类的低速设备。因为寄存器远小于主存，所以在寄存器上处理器读取数据的速度比主存快很多，并且这种差距还在持续增大。而根据局部性原理可知，程序具有访问局部区域内的数据和代码的趋势，所以在处理器和一个较大较慢的设备之间插入一个更小更快的存储设备，来暂时保存处理器近期可能会需要的数据，使得大部分的内存操作都能在高速缓存内完成，就能极大提高系统速度了，而这个设备称为<strong>高速缓存存储器</strong>。</li><li>在单处理器系统中，一般含有二级缓存，最小的<strong>L1高速缓存</strong>速度几乎和访问存储器相当，大一些的<strong>L2高速缓存</strong>通过特殊总线连接到处理器，虽然比L1高速缓存慢，但是还是比直接访问主存来的快。在多核处理器中，还有一个<strong>L3高速缓存</strong>，用来共享多个核之间的数据。一般利用了高速缓存的程序会比没有使用高速缓存的程序的性能提高一个数量级。</li></ul><h1 id="存储器层次"><a href="#存储器层次" class="headerlink" title="存储器层次"></a>存储器层次</h1><p>高速缓存的思想其实不仅仅能应用于CPU中，其实对其进行扩展，就能将计算机系统中的存储设备都组织成一个存储器层次结构，如下图所示</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/4.jpg" style="zoom: 50%;" /><p>存储器层次结构的主要思想是将上一层的存储器作为下一层存储器的高速缓存。程序员可以利用对整个存储器层次结构的理解来提高程序性能。</p><h1 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h1><p>操作系统的出现避免了程序员直接去操作硬件（主存、处理器、I/O设备），它可以看成是应用程序和硬件之间的一层软件，给程序员提供硬件的抽象。比如将正在运行的程序抽象为进程；将程序操作的主存抽象为虚拟内存；将各种I/O设备抽象为文件的形式，让程序员能够直接通过这层软件很好地调用硬件，避免了过多的硬件细节。接下来将简单介绍这三层抽象。</p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/5.png" style="zoom:50%;" /><ul><li>**进程(processes)**：</li></ul><p>为了方便对运行程序时所需的硬件进行操作，操作系统对正在运行的程序提供了一种抽象——<strong>进程</strong>。<strong>提供了一种错觉：</strong>一个系统上可以同时运行多个进程，而每个进程好像在独占地使用硬件。这样程序员就无需考虑程序之间切换所需操作的硬件，这些由操作系统的内核进行管理。</p><p>操作系统通过交错执行若干个程序的指令，不断地在进程间进行切换来提供这种错觉，这个称为<strong>并发运行</strong>。<img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/6.png" style="zoom:67%;" /></p><p>现代系统中，一个进程中可以并发多个线程，每条线程并行执行不同的任务，线程是操作系统能够进行运算调动的最小单位，是进程中的实际运作单位，相比于进程：1）多线程之间比多进程之间更容易共享数据。2）线程一般来说都比进程更高效。</p><blockquote><p>注意：这里一个进程中可以并发多个线程，指的是一个进程一次只能运行一个线程，但是一个进程可以同时含有多个线程，每个线程可以执行不同的任务，进程让线程之间快速切换来达到并发线程。</p></blockquote><ul><li><p><strong>虚拟存储器(Virtual Memory)</strong></p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/7.png" style="zoom: 50%;" /><p>计算机会将多个程序的指令和数据保存在内存中，当某个程序的数据增时，可能不会保存在内存的连续地址中，这就使得代码需要对这些在内存中非连续存储的数据进行读取，会造成很大的困难。</p><p>为了解决这个问题，操作系统对内存和I/O设备进行抽象——<strong>虚拟内存</strong>。<strong>它提供了一种错觉：</strong>程序运行在从0开始的连续虚拟内存空间中，而操作系统负责将程序的虚拟内存地址投影到对应的真实物理内存中。这样使得程序员能直接对连续的空间地址进行操作，而无需考虑非连续的物理内存地址。</p><p>操作系统将进程的虚拟内存划分为多个区域，每个区域都有自己的功能，接下来从最低的地址开始介绍：</p><ul><li>程序代码和数据：对于所有进程来说，代码都是从同一固定地址开始，紧接着是全局变量相对应的数据位置。代码和数据区在进程一开始运行时就被规划了大小。</li><li>堆(Heap)：当调用<code>malloc</code>和<code>free</code>时，堆可以在运行时动态的扩展和收缩</li><li>共享库(Shared libraries)：存放像C标准库和数学库这样共享库的代码和数据的区域</li><li>栈(Stack)：编译器用它来实现函数调用。当调用一个函数时，栈增长；从一个函数返回时，栈收缩</li><li>内存虚拟存储器(Kernel virtual memory)：内核总是驻留在内存当中，是操作系统的一部分。该区域<strong>不允许</strong>应用程序读写或者直接调用其中的函数。</li></ul></li><li><p>**文件(Files)**：操作系统将所有I/O设备看成是文件，而文件是字节序列，这样系统中的所有输入输出可以调用系统函数来读写文件实现，简化了对各种各样的I/O设备的操作。</p></li></ul><h1 id="网络通信"><a href="#网络通信" class="headerlink" title="网络通信"></a>网络通信</h1><p>在后续的课程会详细介绍，这里不展开。</p><p><img src="https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/8.png"></p><h1 id="并发和并行"><a href="#并发和并行" class="headerlink" title="并发和并行"></a>并发和并行</h1><p><strong>并发（Concurrency）</strong>指一个同时具有多个活动的系统。<strong>并行（Paralleism）</strong>指的是用并发来时一个系统运行得更快。并行可以在计算机系统的多个抽象层次上运用。</p><ul><li><p>线程级并发</p></li><li><p>指令级并行</p></li><li><p>单指令、多数据并行</p></li></ul><p>（这一部分内容看书没咋看明白，先放着）</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python开发环境搭建</title>
    <link href="/2019/10/28/Win10%E7%B3%BB%E7%BB%9F%E4%B8%8Bpython%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%88%E6%90%AD%E9%85%8DVsCode%E4%BD%BF%E7%94%A8%EF%BC%89/"/>
    <url>/2019/10/28/Win10%E7%B3%BB%E7%BB%9F%E4%B8%8Bpython%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%88%E6%90%AD%E9%85%8DVsCode%E4%BD%BF%E7%94%A8%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="安装Python"><a href="#安装Python" class="headerlink" title="安装Python"></a>安装Python</h1><h2 id="下载并安装minicaonda"><a href="#下载并安装minicaonda" class="headerlink" title="下载并安装minicaonda"></a>下载并安装minicaonda</h2><p>conda是一个开源的软件包管理工具和环境管理系统，可以轻松创建虚拟环境，并在多个环境之间轻松切换。例如，我们可以分别创建Python3和Python2的独立软件环境，并在需要该环境时轻松进行切换（根据工作目录）。</p><p>与 conda有关的配置软件有两个，miniconda（含基本工具占用空间约400M），anaconda（包括常用到的python库以及GUI工具，占用空间约3G）。因为我们只是需要conda这个包管理工具，仅需要下载miniconda即可。</p><p>直接下载<a href="https://conda.io/miniconda.html">官方miniconda安装包</a>，在下载页面选择对应平台的安装包进行安装。 自带Python环境，可以根据自己长期使用的版本进行选择。下载好安装包后，安装时可以选择作为用户软件安装（储存到C盘下的<code>User/用户文件夹</code>），或者做为系统软件安装（所有用户都可以使用）</p><p>注意安装过程进行到到下面这一步时，需要勾选两个选项：添加miniconda至环境变量PATH，注册miniconda所带的python.exe为默认解释器</p><p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/miniconda.png"></p><h2 id="安装第三方库的方法"><a href="#安装第三方库的方法" class="headerlink" title="安装第三方库的方法"></a>安装第三方库的方法</h2><ul><li>默认的conda源在国外，下载Python第三方包时由于大陆互联网的管制会很慢。因此我们可以使用清华的镜像，执行以下命令：</li></ul><pre><code class="hljs text">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes</code></pre><p>此时，在本地目录<code>C:\Users&lt;你的用户名&gt;</code>下就会生成配置文件<code>.condarc</code>，可以查看conda的所有源。</p><p>如果想要还原conda默认源，执行以下命令：</p><pre><code class="hljs python">conda config --remove-key channels</code></pre><ul><li>除了conda，Python还有一个自带的包管理工具pip，也可以将其替换为国内源，直接在user目录中创建一个pip目录，如：C:\Users\xxxx\pip，新建文件pip.ini，内容如下：</li></ul><pre><code class="hljs text">[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple</code></pre><ul><li>如果遇到在线下载一直失败（网速太慢），或者选择镜像源没有对应的包，可以尝试去以下两个网址下载离线文件进行解压：</li></ul><p><a href="https://repo.anaconda.com/pkgs/main/win-64/">https://repo.anaconda.com/pkgs/main/win-64/</a></p><p><a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#ecos">https://www.lfd.uci.edu/~gohlke/pythonlibs/#ecos</a></p><h2 id="虚拟环境的使用"><a href="#虚拟环境的使用" class="headerlink" title="虚拟环境的使用"></a>虚拟环境的使用</h2><p>虚拟环境的好处可以使我们在创建不同的Python项目时，避免相互之间的影响。</p><p>若我现在需要做一个项目使用 python3.6 的环境，可以在终端运行以下命令</p><pre><code class="hljs text">conda create --name 项目名 python=3.6</code></pre><p>创建环境的过程中会让你确认该环境所需软件包，一般直接输入y后按回车就可以了。环境创建在miniconda安装路径下的<code>\env</code>子文件夹下：</p><p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/env_location.JPG"></p><p><a href="https://blog.csdn.net/lyy14011305/article/details/59500819">点此查看更多虚拟环境的使用方法</a></p><h1 id="VsCode配置"><a href="#VsCode配置" class="headerlink" title="VsCode配置"></a>VsCode配置</h1><p><strong>Win10下最为省事的还是Pycharm，不过Pycharm比较吃内存我还是喜欢用轻量化的Vscode，并且Vscode有丰富的插件支持</strong></p><p>在windows平台下，由于VsCode打开的终端默认支持的是powershell，单因为某些原因在powershell内输入激活conda所创建的虚拟环境命令，并不能成功生效</p><ul><li>一种解决办法是打开VsCode的terminal窗口，将默认的shell手动修改为cmd。重启之后终端输入<code>conda activate 虚拟环境名</code>激活所创建的虚拟环境</li></ul><p>输入<code>conda deactivate</code>退出虚拟环境。</p><ul><li>当然，如果熟悉在linux下工作的指令，可以尝试将Vscode的默认终端改成gitbash，gitbash可以通过下载git软件来获取。</li></ul><p>下载好git后打开VsCode的用户设置文件<code>ctrl+shifp+p</code>，修改<code>setting.json</code>文件增加如下两行：</p><p><img src="https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main/img/modify_setting.JPG"></p><p>重启VsCode后终端默认调用的shell就变成了gitbash：</p><p>不过此时还需要一个步骤，在终端首先输入<code>source activate</code>，可以看到此时已经激活了conda，这是还是base环境即默认的Python解释器</p><p>终端输入<code>conda activate 虚拟环境名</code>激活所创建的虚拟环境</p><h1 id="Jupyterlab"><a href="#Jupyterlab" class="headerlink" title="Jupyterlab"></a>Jupyterlab</h1><p><a href="https://zhuanlan.zhihu.com/p/154515490">https://zhuanlan.zhihu.com/p/154515490</a></p><h1 id="深度学习平台的配置"><a href="#深度学习平台的配置" class="headerlink" title="深度学习平台的配置"></a>深度学习平台的配置</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VsCode</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo博客搭建指南</title>
    <link href="/2019/10/21/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <url>/2019/10/21/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="Hexo简介"><a href="#Hexo简介" class="headerlink" title="Hexo简介"></a>Hexo简介</h1><p>Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub上，是搭建博客的首选框架。更多详细信息可以进入<a href="https://link.zhihu.com/?target=https://hexo.io/zh-cn/">hexo官网</a>进行查看，Hexo的创建者是台湾人，对中文的支持很友好。</p><h1 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h1><h2 id="注册github账户"><a href="#注册github账户" class="headerlink" title="注册github账户"></a>注册github账户</h2><p>因为需要使用github page来托管我们的网站（当然后续也可以通过申请个人域名），因此首先你需要注册一个github账户，登录github网站按照网站提示的步骤进行操作。</p><h2 id="本地安装并配置git"><a href="#本地安装并配置git" class="headerlink" title="本地安装并配置git"></a>本地安装并配置git</h2><p>Git是目前世界上最先进的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。也就是用来管理你的hexo博客文章，上传到GitHub的工具。Git非常强大，作为程序开发人员都应掌握git的使用。廖雪峰老师的<a href="https://link.zhihu.com/?target=https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000">Git教程</a>写的非常好</p><p>在git官网下载安装 git for windows，下载后会有一个git bash的命令行工具，以后就用这个工具来使用git，在git bash执行：</p><pre><code class="hljs text">git config --global user.name &quot;xiangli-bjtu&quot;  // 换成自己的github用户名，非昵称 git config --global user.email  &quot;xavierlee.gy@gmail.com&quot;  // 填写自己的github注册邮箱</code></pre><h2 id="配置ssh免密登录"><a href="#配置ssh免密登录" class="headerlink" title="配置ssh免密登录"></a>配置ssh免密登录</h2><p>提交代码需要提供你的github权限才可以，但是直接使用用户名和密码太不安全了，所以我们使用ssh key来解决本地和服务器的连接问题。右键git bash执行以下命令：</p><pre><code class="hljs text">ssh-keygen -t rsa -C &quot;xavierlee.gy@gmail.com&quot;</code></pre><p>然后连续3次回车，会在用户目录下生成一个<code>.ssh</code>文件夹里面有公钥、私钥、主机列表三个文件。打开<code>.ssh\id_rsa.pub</code>文件，这个就是公钥。然后进入你的github主页，进入<code>个人设置 → SSH and GPG keys → New SSH key</code>将刚复制的内容粘贴到key那里，title随便填，保存。</p><p>输入以下命令检查是否配置成功：</p><pre><code class="hljs text">ssh -T git@github.com</code></pre><p>出现提示，输入yes，如果之后看到<code>You&#39;ve successfully authenticated, but GitHub does not provide shell access.</code>说明SSH已配置成功！</p><p><a href="https://www.jianshu.com/p/33461b619d53">关于ssh的原理</a></p><h2 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node.js"></a>安装node.js</h2><p>创建博客的hexo工具基于node.js，所以需要安装一下node.js和包管理工具npm，直接去官网下载安装即可。</p><p>检测npm是否安装成功，在命令行中输入<code>npm -v </code>。另外，windows在git安装完后，就可以直接使用git bash来敲命令行了，不用自带的cmd</p><p>由于默认的npm源在国外，需要更换npm源为淘宝源：</p><pre><code class="hljs text">npm config set registry https://registry.npm.taobao.org</code></pre><p>在用户目录下会生成一个<code>.npmrc</code>文件可以查看使用的源。</p><h1 id="Hexo博客搭建"><a href="#Hexo博客搭建" class="headerlink" title="Hexo博客搭建"></a>Hexo博客搭建</h1><h2 id="安装hexo和创建博客目录"><a href="#安装hexo和创建博客目录" class="headerlink" title="安装hexo和创建博客目录"></a>安装hexo和创建博客目录</h2><ol><li>hexo是一个基于node.js的工具，能将用于撰写博客的markdown文档渲染为网页。在电脑一个合适的位置创建一个文件夹，可以命名为blog，hexo框架与博客有关内容都在这个文件夹中，进入这个文件夹下直接右键git bash打开，输入以下命令安装hexo。</li></ol><pre><code class="hljs text">npm install -g hexo-cli</code></pre><p>接下来进行博客的初始化</p><pre><code class="hljs text">hexo init</code></pre><p>初始化完成之后，我们可以看到多个文件目录：</p><ul><li><code>node_modules</code>: 依赖包</li><li><code>public</code>：存放生成的页面</li><li><code>scaffolds</code>：生成文章的一些模板</li><li><code>source</code>：用来存放你的文章</li><li><code>themes</code>：主题</li><li><code>_config.yml</code>: 博客的配置文件</li></ul><p>执行</p><pre><code class="hljs text">hexo cleanhexo generate</code></pre><p>其中 <code>hexo clean</code>清除了你之前生成的东西，也可以不加，更为简略的命令是使用缩写用 <code>hexo c</code></p><p> <code>hexo generate</code> 顾名思义生成静态文章，可以用 <code>hexo g</code>缩写</p><p>将本地的markdown文件编译成静态文件（即显示网页必须的内容如html、css、js等）。这些内容会保存博客文件夹内。接下来执行<code>hexo s</code> 测试hexo是否安装成功，打开浏览器访问 <a href="http://localhost:4000/">http://localhost:4000</a> 若能看到内容则说明安装成功</p><p>使用<code>ctrl+c</code>可以把服务关掉。</p><h2 id="部署到github"><a href="#部署到github" class="headerlink" title="部署到github"></a>部署到github</h2><p>上面只是在本地生成了博客文件，接下来要做的就是就是把博客内容发布到github上，这样就可以让其他的人进行访问。我们需要安装插件，用来将本地hexo博客的内容部署到github</p><pre><code class="hljs text">npm install hexo-deployer-git --save</code></pre><p>在github创建一个名为<code>xiangli-bjtu.github.io</code>的远程仓库，如果你的github用户名是test，那么你就新建<code>test.github.io</code>的仓库（必须是你的用户名，这是固定要求其它名称无效），将来你的网站访问地址就是 <a href="http://test.github.io/">http://test.github.io</a> 了</p><p>接下来我们需要修改本地hexo博客目录的配置文件<code>_config.yml</code>中有关deploy的部分</p><pre><code class="hljs text">deploy:  type: git    repository: git@github.com:xiangli-bjtu/xiangli-bjtu.github.io.git  branch: main  # 2021年修改：由于美国弗洛伊德事件引发的BLM运动，github已将默认分支的名称由原来带有正义的master修改为main</code></pre><p>然后执行</p><pre><code class="hljs text">hexo cleanhexo generatehexo deploy</code></pre><p>就可以将本地public文件夹下的内容上传部署到github，过一会儿就可以在<code>xiangli-bjtu.github.io</code>这个网站看到你的博客了。</p><h1 id="Hexo配置"><a href="#Hexo配置" class="headerlink" title="Hexo配置"></a>Hexo配置</h1><p>在文件根目录下的<code>_config.yml</code>，就是整个hexo框架的配置文件了。可以在里面修改大部分的配置。详细可参考<a href="https://link.zhihu.com/?target=https://hexo.io/zh-cn/docs/configuration">官方的配置</a>描述。</p><p>下面介绍几个常用的配置：</p><h2 id="网站信息"><a href="#网站信息" class="headerlink" title="网站信息"></a><strong>网站</strong>信息</h2><p><code>title</code>：网站标题</p><p><code>subtitle</code>网站副标题</p><p><code>description</code>网站描述，主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，通常建议在其中包含您网站的关键词。</p><p><code>author</code>您的名字</p><p><code>language</code>网站使用的语言</p><p><code>timezone</code>网站时区。Hexo 默认使用您电脑的时区。<a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">时区列表</a>。比如说：<code>America/New_York</code>, <code>Japan</code>, 和 <code>UTC</code> 。</p><h2 id="博客图片内容管理"><a href="#博客图片内容管理" class="headerlink" title="博客图片内容管理"></a>博客图片内容管理</h2><p>修改本地hexo博客目录下站点配置文件<code>_config.yml</code>中的<code>post_asset_folder:false</code>这个选项设置为<code>true</code>。这样在运行<code>hexo n &quot;xxxx&quot;</code>来生成marnkown文时，<code>/source/_posts</code>文件夹内除了xxxx.md文件还有一个同名的文件夹，用于存放markdown文件里需要插入的图片</p><h2 id="Front-matter"><a href="#Front-matter" class="headerlink" title="Front-matter"></a><strong>Front-matter</strong></h2><p> <a href="https://hexo.io/zh-cn/docs/front-matter">Front-matter</a>  是markdown文件最上方以 <code>---</code> 分隔的区域，用于指定个别文件的变量，举例来说：</p><pre><code class="hljs text">title: Hello Worlddate: 2013/7/13 20:46:25---</code></pre><p>下是预先定义的参数，您可在模板中使用这些参数值并加以利用。</p><p><code>layout</code> 布局</p><p><code>title </code>文章标题</p><p><code>date </code>建立日期</p><p><code>updated </code>更新日期</p><p><code>comments </code>开启文章的评论功能</p><p><code>tags </code>标签</p><p><code>categories </code>分类</p><p><code>permalink </code>覆盖文章网址</p><p>其中分类和标签的区别在于：分类具有顺序性和层次性，也就是说 <code>Foo, Bar</code> 不等于 <code>Bar, Foo</code>；而标签没有顺序和层次。</p><h2 id="3种layout（布局）"><a href="#3种layout（布局）" class="headerlink" title="3种layout（布局）"></a>3种<strong>layout（布局）</strong></h2><p>Hexo 有三种默认布局：<code>post</code>、<code>page</code> 和 <code>draft</code>。在创建这三种不同类型的文件时，它们将会被保存到不同的路径：</p><table><thead><tr><th align="left">布局</th><th align="left">路径</th></tr></thead><tbody><tr><td align="left"><code>post</code></td><td align="left"><code>source/_posts</code></td></tr><tr><td align="left"><code>page</code></td><td align="left"><code>source</code></td></tr><tr><td align="left"><code>draft</code></td><td align="left"><code>source/_drafts</code></td></tr></tbody></table><ul><li>默认的布局是<code>post</code>，当你每一次使用代码</li></ul><pre><code class="hljs text">hexo n</code></pre><p>它其实默认使用了<code>post</code>布局，也就是在<code>source</code>文件夹下的<code>_post</code>里面生成博客文件</p><ul><li>如果你想另起一页，那么可以使用<code>page</code>布局</li></ul><pre><code class="hljs text">hexo new page board</code></pre><p>系统会自动给你在source文件夹下创建一个board文件夹，以及board文件夹中的index.md，这样你访问的board对应的链接就是<code>http://xxx.xxx/board</code></p><ul><li><p><code>draft</code>是Hexo 的一种特殊布局，也就是你如果想写文章，又不希望发布时被看到，可以使用该布局。若在写草稿文件的过程中，想要预览一下，那么可以使用以下命令，在本地端口中开启服务预览。</p><pre><code class="hljs text">hexo server --draft</code></pre><p>如果想把草稿转为post，可通过 <code>publish</code> 命令将草稿移动到 <code>source/_posts</code> 文件夹，该命令的使用方式与 <code>new</code> 十分类似</p><pre><code class="hljs text">hexo publish draft newpage</code></pre><h2 id="更换Hexo主题"><a href="#更换Hexo主题" class="headerlink" title="更换Hexo主题"></a>更换Hexo主题</h2></li></ul><p>如果对原始的hexo主题不满意，hexo还提供了丰富的主题进行选择，这里我使用的是<a href="https://fluid-dev.github.io/hexo-fluid-docs/start/#%E6%90%AD%E5%BB%BA-hexo-%E5%8D%9A%E5%AE%A2">fluid主题</a>。这里仅介绍几个用到的主题配置步骤，更多的可以去看fluid主题的官网文档</p><h3 id="安装主题"><a href="#安装主题" class="headerlink" title="安装主题"></a><strong>安装主题</strong></h3><p>下载<a href="https://codeload.github.com/fluid-dev/hexo-theme-fluid/zip/v1.8.4">fluid</a>解压到 themes 目录，并将解压出的文件夹重命名为 <code>fluid</code>。</p><p>修改博客目录下的站点配置文件 <code>_config.yml</code></p><pre><code class="hljs text">theme: fluid</code></pre><h3 id="页面顶部大图"><a href="#页面顶部大图" class="headerlink" title="页面顶部大图"></a><strong>页面顶部大图</strong></h3><ul><li>图源</li></ul><p><strong>主题配置文件</strong>中，每个页面都有名为 <code>banner_img</code> 的属性，可以使用本地图片的相对路径，也可以为外站链接，比如：</p><p>指向本地图片：</p><pre><code class="hljs text">banner_img: /img/bg/example.jpg   # 对应存放在 /source/img/bg/example.jpg</code></pre><p>本地图片时可自定义路径，但必须在 source 目录下。图片大小建议压缩到 1MB 以内，否则会严重拖慢页面加载。</p><p>指向外站链接：</p><pre><code class="hljs text">banner_img: https://static.zkqiang.cn/example.jpg</code></pre><ul><li>高度</li></ul><p>鉴于每个人的喜好不同，开放对页面 <code>banner_img</code> 高度的控制。</p><p><strong>主题配置文件</strong>中，每个页面对应的 <code>banner_img_height</code> 属性，有效值为 0 - 100。100 即为全屏，个人建议 70 以上。</p><ul><li>蒙版透明度</li></ul><p><strong>主题配置文件</strong>中，每个页面对应的 <code>banner_mask_alpha</code> 属性，有效值为 0 - 1.0， 0 是完全透明（无蒙版），1 是完全不透明</p><h3 id="博客标题"><a href="#博客标题" class="headerlink" title="博客标题"></a><strong>博客标题</strong></h3><p>页面左上角的博客标题，默认使用<strong>博客配置</strong>中的 <code>title</code>，这个配置同时控制着网页在浏览器标签中的标题。</p><p>如需单独区别设置，可在<strong>主题配置</strong>中设置：</p><pre><code class="hljs text">navbar:  blog_title: 记录学习的点滴 # 导航栏左侧的标题，为空则按 hexo config.title 显示</code></pre><h3 id="文章在首页的略缩图"><a href="#文章在首页的略缩图" class="headerlink" title="文章在首页的略缩图"></a>文章在首页的略缩图</h3><p>markdown文章开头的 <code>Front-matter</code> ，可添加 <code>index_img</code> 属性。</p><pre><code class="hljs text">---title: 文章标题tags: [Hexo, Fluid]index_img: /img/example.jpgdate: 2019-10-10 10:00:00---文章内容</code></pre><p>和 Banner 配置相同，<code>/img/example.jpg</code> 对应的是存放在 <code>/source/img/example.jpg</code> 目录下的图片（目录也可自定义，但必须在 source 目录下）。也可以使用外链 Url 的绝对路径。</p><h3 id="创建「关于页」"><a href="#创建「关于页」" class="headerlink" title="创建「关于页」"></a>创建「关于页」</h3><p>在Fluid主题界面的菜单栏里，有一个about页面「关于页」，这个你现在点击进去时找不到网页的，因为你的文章中没有about这个东西。如果你想要的话，需要手动创建：</p><pre><code class="hljs text">hexo new page about</code></pre><p>它就会在根目录下<code>source</code>文件夹中新建了一个<code>about</code>文件夹，以及<code>index.md</code>。修改 <code>index.md</code>，为其添加 <code>layout</code> 属性，并在里面写s</p><p>修改后的文件示例如下：</p><pre><code class="hljs text">---title: aboutdate: 2019-10-21 21:23:01layout: about---这里写关于页的正文，支持 Markdown, HTML</code></pre><h3 id="支持Latex公式"><a href="#支持Latex公式" class="headerlink" title="支持Latex公式"></a>支持Latex公式</h3><p>当需要使用 Latex 语法的数学公式时，可手动开启本功能，需要完成三步操作：</p><p><strong>1. 设置主题配置</strong></p><pre><code class="hljs text">post:  math:    enable: true    specific: true    engine: mathjax</code></pre><p><code>specific</code>: 建议开启。当为 true 时，只有在文章 <code>Front-matter</code> 里指定 <code>math: true</code> 才会在文章页启动公式转换，以便在页面不包含公式时提高加载速度。</p><p><code>engine</code>: 公式渲染引擎，目前支持 <code>mathjax</code> 或 <code>katex</code>。</p><p><strong>MathJax</strong></p><p>优点</p><ul><li>对 LaTeX 语法支持全面</li><li>右键点击公式有扩展功能</li></ul><p>缺点</p><ul><li>需要加载 JS，页面加载会比较慢，并且有渲染变化</li><li>kramed 渲染器对内联公式的转义字符 <code>\</code> 支持不足</li></ul><p><strong>KaTeX</strong></p><p>优点</p><ul><li>没有 JS 不会影响页面加载</li><li>渲染器效果好 (相对 kramed 对 MathJax 的内联公式)</li></ul><p>缺点</p><ul><li>小部分 LaTeX 不支持</li></ul><p><strong>2. 更换 Markdown 渲染器</strong></p><p>由于 Hexo 默认的 Markdown 渲染器不支持复杂公式，所以必须更换渲染器。</p><p>先卸载原有渲染器：</p><pre><code class="hljs text">npm uninstall hexo-renderer-marked --save</code></pre><p>然后根据上方配置不同的 <code>engine</code>，推荐更换如下渲染器：</p><p>mathjax: <code>npm install hexo-renderer-kramed --save</code></p><p>katex: <code>npm install @upupming/hexo-renderer-markdown-it-plus --save</code></p><h3 id="社交链接"><a href="#社交链接" class="headerlink" title="社交链接"></a>社交链接</h3><p>在主题配置中设置：</p><pre><code class="hljs yaml"><span class="hljs-attr">about:</span>  <span class="hljs-attr">icons:</span> <span class="hljs-comment"># 更多图标可从 https://hexo.fluid-dev.com/docs/icon/ 查找，class 代表图标的 css class</span>    <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">class:</span> <span class="hljs-string">&#x27;iconfont icon-github-fill&#x27;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&#x27;https://github.com/xiangli-bjtu&#x27;</span>, <span class="hljs-attr">tip:</span> <span class="hljs-string">&#x27;GitHub&#x27;</span> &#125;    <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">class:</span> <span class="hljs-string">&#x27;iconfont icon-bilibili-fill&#x27;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&#x27;https://space.bilibili.com/254950760&#x27;</span>, <span class="hljs-attr">tip:</span> <span class="hljs-string">&#x27;b站&#x27;</span> &#125;    <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">class:</span> <span class="hljs-string">&#x27;iconfont icon-zhihu-fill&#x27;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&#x27;https://www.zhihu.com/people/li-xiang-43-89-60&#x27;</span>, <span class="hljs-attr">tip:</span> <span class="hljs-string">&#x27;知乎&#x27;</span> &#125;    <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">class:</span> <span class="hljs-string">&#x27;iconfont icon-wechat-fill&#x27;</span>, <span class="hljs-attr">qrcode:</span> <span class="hljs-string">&#x27;/img/wechat_QRcode.jpg&#x27;</span> &#125;</code></pre><ul><li><code>class</code>: 图标的 css class，主题内置图标详见<a href="https://fluid-dev.github.io/hexo-fluid-docs/icon/">这里</a></li><li><code>link</code>: 跳转链接</li><li><code>tip</code>: 鼠标悬浮在图标上显示的提示文字</li><li><code>qrcode</code>: 二维码图片，当使用此字段后，点击不再跳转，而是悬浮二维码（需要指定图片）</li></ul><h1 id="为博客开设图床"><a href="#为博客开设图床" class="headerlink" title="为博客开设图床"></a>为博客开设图床</h1><p>图床简单来说就是个图片仓库，随着我们写博客越来越多，若将博客内插入的图片全都存在本地会很占空间。</p><p>在各家厂商提供的图床服务中，GitHub 图床是个不错的选择，还支持 jsDelivr CDN 加速访问（jsDelivr 是一个免费开源的 CDN 解决方案），我们可以使用PicGo 工具一键上传图片，操作简单高效，GitHub 和 jsDelivr 都是大厂，不用担心跑路问题，不用担心速度和容量问题，而且完全免费，可以说是目前免费图床的最佳解决方案！</p><h3 id="创建Github图床仓库"><a href="#创建Github图床仓库" class="headerlink" title="创建Github图床仓库"></a>创建Github图床仓库</h3><p>首先我们创建仓库，命名为<code>Blog-images-hosting</code></p><h3 id="生成Token"><a href="#生成Token" class="headerlink" title="生成Token"></a>生成Token</h3><p>在主页依次选择【Settings】-【Developer settings】-【Personal access tokens】-【Generate new token】，填写好描述，勾选【repo】，然后点击【Generate token】生成一个Token，注意这个Token只会显示一次，自己先保存下来，或者等后面配置好PicGo后再关闭此网页</p><h3 id="配置PicGo"><a href="#配置PicGo" class="headerlink" title="配置PicGo"></a>配置PicGo</h3><p>PicGo是一个用于快速上传图片并获取图片 URL 链接的工具，前往<a href="https://github.com/Molunerfinn/picgo/releases">下载PicGo</a>，安装好后开始配置图床</p><ul><li><p>设定仓库名：按照【github用户名/图床仓库名】的格式填写</p></li><li><p>设定分支名：【main】</p></li><li><p>设定Token：粘贴之前生成的【Token】</p></li><li><p>指定存储路径：填写想要储存的路径，如【ITRHX-PIC/】，这样就会在仓库下创建一个名为 ITRHX-PIC 的文件夹，图片将会储存在此文件夹中</p></li><li><p>设定自定义域名：它的作用是在图片上传后生成访问链接，自定义域名需要按照这样去填写：</p><p><code>https://raw.githubusercontent.com/账户名/仓库名/main</code>，</p><p>比如我的是：<code>https://raw.githubusercontent.com/xiangli-bjtu/Blog-images-hosting/main</code></p></li></ul><h3 id="Typora集成PicGo"><a href="#Typora集成PicGo" class="headerlink" title="Typora集成PicGo"></a>Typora集成PicGo</h3><p>Hexo博客文档是markdown文件，推荐的一款编辑工具是Typora。点击<code>偏好设置 -&gt; 图像 -&gt;</code>，在上传服务，选择 PicGo(app)。PicGo路径，选择PicGo软件的安装路径。点击验证图片上传选项可以测试是否配置成功。</p><h1 id="实现在多终端上更新博客"><a href="#实现在多终端上更新博客" class="headerlink" title="实现在多终端上更新博客"></a>实现在多终端上更新博客</h1><p>hexo 是一个优秀的静态博客工具，唯一的不足就是源文件无法同步，让人几乎只能在一台电脑上写博客，为了解决这个问题，我们可以利用git的分支系统进行多终端工作了，这样每次打开不一样的电脑，只需要进行简单的配置和在github上把文件同步下来，就可以无缝操作了。</p><p>由于<code>hexo d</code>上传部署到github的其实是hexo编译后的文件（即博客目录下的public文件夹内容），是用来生成网页的，不包含源文件。</p><p>因此我们的思路就是利用git的分支管理，将源文件上传到github的另一个分支即可。</p><ul><li>首先在github仓库中新建一个分支<code>hexo-source</code>，用这个分支来存储博客的源文件，而<code>main</code>分支用来存放hexo编译后的网页文件。然后在这个仓库的settings中，选择默认分支为<code>hexo-source</code>（这样每次同步的时候就不用指定分支，比较方便）。</li><li>然后在本地的任意目录下，打开git bash，将默认分支克隆到本地</li></ul><pre><code class="hljs text">git clone git@github.com:xiangli-bjtu/xiangli-bjtu.github.io.git</code></pre><ul><li><p>将上述克隆到本地的仓库中，除了.git 文件夹外的所有文件都删掉</p><p>把之前我们写的博客源文件全部复制过来，除了<code>.deploy_git</code>。这里应该说一句，复制过来的源文件应该有一个<code>.gitignore</code>，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下，表示这些类型文件不需要git：</p><pre><code class="hljs text">.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/</code></pre></li><li><p>然后</p></li></ul><pre><code class="hljs text">git add .# 将hexo源文件映射到远程repo上git commit -m &#x27;commit source files&#x27;# 将源文件push到分支git push</code></pre><p>然后就可以在github上看到新建了分支，里面已经有博客的源文件了。</p><p>这样，我们在<code>source</code>文件夹中写好一篇markdown文件后，博客的部署分为两步：</p><ol><li>当添加新文章或更改配置后，需要将hexo源文件push到分支<code>hexo-source</code>进行备份</li></ol><pre><code class="hljs text">git add .  //添加修改内容到本地仓储git commit -m &#x27;modify blog&#x27;  //提交修改内容到本地仓库git push  //将本地分支和分支下的内容推送到远程</code></pre><ol start="2"><li>接下来执行博客编译将网页静态文件上传:</li></ol><pre><code class="hljs text">hexo chexo d -g</code></pre><h2 id="更换设备后迁移博客"><a href="#更换设备后迁移博客" class="headerlink" title="更换设备后迁移博客"></a>更换设备后迁移博客</h2><p>如果我们换了一台电脑，配置好 Hexo 的环境，<a href="https://hsiaovv.github.io/2017/04/06/GitHub%E9%85%8D%E7%BD%AESSH-key/">配置 Git SSH key</a>，把博客源文件代码克隆下来:</p><pre><code class="hljs crmsh">git <span class="hljs-keyword">clone</span> <span class="hljs-title">xxxxxxxxx</span>.xx (你的 github page 的 repo 地址)</code></pre><p>博客源文件下载下来之后，默认的分支是 master，需要切换到 <code>blogSource</code> 分支</p><pre><code class="hljs maxima">git checkout <span class="hljs-built_in">origin</span>/blogSource</code></pre><p>然后cd到博客目录依次执行以下命令：</p><pre><code class="hljs cmake">npm <span class="hljs-keyword">install</span> hexonpm <span class="hljs-keyword">install</span>npm <span class="hljs-keyword">install</span> hexo-deployer-git --save</code></pre><p>接下来就可以开始愉快的写博客了，写完之后记得把源文件代码 push 到 Github 上，然后用 Hexo 部署到自己博客上面</p><h1 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h1><p>待补充……</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
