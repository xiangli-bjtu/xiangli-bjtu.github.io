---
title: '[论文阅读01] Neurosurgeon：Collaborative Intelligence Between the Cloud and Mobile Edge'
date: 2019-12-16 15:04:09
tags:
- 边缘计算
categories:
- 论文阅读
---

> 论文出处：2017 **ASPLO** (Architectural Support for Programming Languages and Operating Systems)会议，是计算机体系结构方向的顶级学术会议，[《Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge》](https://web.eecs.umich.edu/~jahausw/publications/kang2017neurosurgeon.pdf)



# 1. Introduction 

目前越来越多的智能应用程序，使用语音和图像类型的数据作为输入。比如智能个人助理（Apple Siri、Microsoft Cortana），智能家居和可穿戴设备、导航应用等，人们与移动设备交互模式正在发生迅速改变，预计将取代传统的基于文本的输入形式。

这些应用的实现依赖准确且高度复杂的机器学习技术，其中最常见的是深度神经网络（DNN）。早期的工作认为：传统的移动设备不能支持这种大量的计算，以满足合理的延迟和能量消耗。Web服务提供商用于智能应用程序的现行方法，是把用户的移动设备生成的查询发送到云进行处理，在远端的高性能云服务器上托管所有计算。然而利用这种方法，大量数据（例如，图像，视频和音频）经由无线网络和回程链路上载到服务器，导致高等待时间和能量成本。

庆幸的是，现代移动硬件的性能和能效通过强大的移动SoC集成继续得到改善。受此启发，本文的作者重新审视了移动和云之间智能应用程序的计算细分。即在运行以神经网络为支撑的应用程序任务时，将计算任务拆解为server-side和edge-side两部分，其中server-side在数据中心的服务器里执行，edge-side则在终端设备上执行。

![](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/1.PNG)

这项工作要处理的主要问题包括：

1. 在当今的移动平台上执行大规模智能工作负载的可行性如何？
2. 在何种情况下，通过无线网络传输语音和图像数据的成本过高，而无法发挥云处理的合理性？
3. 在为需要大量计算的智能应用程序提供处理支持方面，移动边缘应当扮演什么角色？

作者使用8个基于DNN的智能应用程序进行试验，这些应用程序涵盖了视觉，语音和自然语言领域。作者发现：终端设备与数据中心的网络连接带宽，设备的计算能力，DNN模型的结构都会影响到计算任务的拆解方式。拆解的粒度是神经网络的layer。这种任务划分策略的考虑，可以影响端到端延迟和移动能源效率。这也可以被认为是一个带约束的优化问题，优化目标是对计算任务在两个计算结点（终端和服务器端）之间进行拆分，约束则包括网络结构、网络带宽以及设备的计算能力。同时，将计算推到云之外的移动设备上，也提高了数据中心的吞吐量，允许给定的数据中心支持更多的用户查询，为移动设备和云系统创造了双赢的局面。

作者为此设计了一个轻量级动态调度系统：Neurosurgeon。它是一个跨越云和移动平台的系统，可自动识别DNN中的理想分区点，并协调移动设备和数据中心之间的计算分配。针对8个DNN应用程序的评估表明，使用Neurosurgeon，平均可将端到端延迟提高3.1倍，将移动能耗降低59.5％，并将数据中心吞吐量提高1.5倍。



# 2. Background

本部分，主要是对深度神经网络（DNN）的概述，作者描述计算机视觉，语音和自然语言处理等应用程序，如何利用DNN作为其核心机器学习算法。

![](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/2.PNG)



#  3. Cloud-only Processing

## Experimental setup

介绍了本文实验中，移动端和服务器端采用的硬件平台：

![](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/3.PNG)

![](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/4.PNG)

以及采用的DNN网络的搭建框架：

![5](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/5.PNG)

## Examining the Mobile Edge

本文作者以经典的AlexNet网络（一个典型的用于图像分类的卷积神经网络）作为示例，网络以152KB图像作为输入。在通信延迟、计算延迟、端到端延迟和能耗方面进行对比。对于无线通信，使用TestMyNet软件在多个移动设备上测量3G，LTE和Wi-Fi的带宽。

![](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/6.PNG)

先看时延这一指标：

- **通信延迟** ：图3a显示了通过3G，LTE和Wi-Fi上传输入图像的延迟。表明网络类型对于实现将数据传送至云端的低延迟至关重要。
- **计算延迟**：图3b显示了在移动CPU，GPU和云GPU上运行AlexNet网络的计算延迟。可以看出在移动设备上运行神经网络的性能还是和云端服务器存在差距；移动设备上GPU处理要强过CPU处理；注意到，即便移动CPUs上运行AlexNet处理图像的时间，仍然比通过3G上传输数据输入快2.3倍。
- **端到端延迟** ：图3c显示了仅在云端处理和仅在移动设备上处理，两种方式所需的总延迟，每个条形顶部的注释是用于计算所花费的端到端延迟的占比。
  - 使用云服务器进行全部的计算时，结果表明计算所消耗的时间仅占全部时间的6%，而剩余的94%都消耗在数据传输上。
  - 只要移动设备拥有可用的GPU，在本地GPU上实施所有的计算操作能够带来最佳的体验（总延迟时间最短）；同时，在LTE和Wi-Fi网络条件下，传输至云端处理要比仅用移动设备CPU进行全部的计算操作要更好（系统延迟时间更少）。
- **能耗**：如果移动设备连接的是Wi-Fi网络，最低的电量损耗方案是发送相应的数据到云服务器并让其进行全部的计算操作。但如果连接的是3G或LTE网络，并且该移动设备有可用的GPU，那么在本地GPU上实施全部的计算操作这一方案所导致的电量消耗，会比数据传输且在云服务器上实施全部的计算操作这一方案更低。



# 4. Fine-grained Computation Partitioning 细粒度计算分区

上一章所有计算都实施在云服务器或移动设备上，这两种“相对极端”的方法之间，或者说在数据传输和实施计算两者之间是否存在一种折中。显然，DNN分层级的网络结构，提供适合于分区计算的抽象概念。

## Layer Taxonomy 

这一小节主要是对目前DNN中存在的各种类型的层和其功能的简要介绍，涉及深度学习有关的知识。包括：Fully-connected Layer、Convolution & Local Layer、Pooling Layer、Activation Layer、normalization layer、softmax layer、argmax layer、dropout layer等。

## Characterizing Layers in AlexNet

首先研究AlexNet中每个层的数据和计算特征（在本节及后续部分中，在移动和服务器平台中均使用GPU），将得到如下的统计图。（浅色柱）显示了移动GPU上按顺序执行AlexNet每个层的延迟，（深色柱）显示了每层输出数据的大小，这也是下一层的输入。

![](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/7.PNG)

* 每一层都有不同的计算时间和数据产生开销。

* 卷积（conv）和全连接层（fc）是最耗时的层，占总执行时间的90％以上。
* 中间的卷积层（conv3和conv4）比早期的卷积层（conv1和conv2）花费更长的时间。这是因为后面的卷积层会使用更多数量的滤波器以提取更强的特征，从而增加计算量。
* AlexNet的前几层会生成大量输出数据，而池化层（pool）会将数据数量锐减，数据数量通过激活层保持不变（relu1-relu5），最后的几层（位于深层的全连接层，softmax，argmax）会逐渐减小数据大小直到最后将数据减少为一个分类标签。

对于AlexNet这种结构的DNN来说，位于网络前端的层产生较大的数据量，而后端的层则带来了较大的计算开销。

## Layer-granularity Computation Partitioning 层粒度计算分区

下面研究如果各个网络层分割开会发生什么呢？（即在移动设备上处理模型的前n层，再把得到的第n层的输出结果传输至云服务器上进行之后的计算，最后再将输出结果传输至移动设备上）在本节中，使用Wi-Fi作为无线网络配置。

在下图中的每个条形，意味着在这一层之后进行分区的端到端延迟和移动能量消耗。那么，最左边的条表示发送原始输入进行云端处理；最右边的条是在移动设备上本地执行整个DNN。

![8](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/8.PNG)

- **延迟分区** ： 如果在前端进行分区，则数据传输主导端到端延迟，这与之前观察到的数据大小在DNN早期阶段的变化趋势是一致的。在后端进行分区可以提供更好的性能，这样可以最大限度地减少数据传输开销，同时后端网络层需要大量的计算，可以利用功能强大的云端服务器执行。在AlexNet使用移动GPU和Wi-Fi的情况下，在pool5和fc6层之间的分区实现了最低延迟，比全部传送到云处理上提高了2.0倍 。
- **能量分区** –：与延迟类似，由于无线数据传输的高能源成本，转移仅用于云的处理的输入并不是最节能的方法。 如图6b所示，DNN中间的分区实现了最佳的移动能耗，比纯云方法节能18％。

## Generalizing to More DNNs

作者进一步将实验扩展到7个更智能的应用程序（7个不同结构的DNN），以研究它们的数据和计算特性及其对计算分区的影响。

![9](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/9.PNG)

对于应用于计算机视觉 (Computer Vision, CV) 领域的具有卷积层的模型，由于卷积层后的数据大小增加，通常最佳的分割点在模型的中部；而对于通常只有全连接层和激活层的网络模型（主要应用在语音识别（Automatic Speech Recognition, ASR）和自然语言处理领域(Natural Language Process, NLP)）而言，在模型的开始部分或者结尾部分进行分割往往更好一点。可以看出，最佳分割点随着模型的不同而变化着。

基于不同DNN结构的应用程序的最佳分区点的变化表明：需要一种系统能够智能地选择划分DNN的最佳点以优化端到端延迟或移动设备能量消耗，并利用云服务器和设备GPU分配相应的计算操作。这也引出了下一章这篇文章的核心工作。



# 5. Neurosurgeon

对于一个DNN模型，影响最佳的分割点位置的因素主要有两种：一种是静态的因素，例如模型的结构；一种是动态的因素，即使对于相同的DNN架构，诸如无线网络状态、数据中心负载和设备剩余可用的电量等的动态因素也会影响最佳分区点。

基于以上的因素，作者为此提出了智能DNN分区引擎Neurosurgeon的设计，由部署阶段和运行时系统组成。

![10](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/10.PNG)

* **部署阶段**：描述移动设备和服务器，以生成DNN常用的各种层类型的性能预测模型，需要注意的是这和应用程序无关的，只需要针对给定的移动和服务器平台进行一次。这组预测模型存储在移动设备上，随后用于预测每层的延迟和能量成本
* **运行阶段** ： 在移动设备上执行基于DNN的智能应用程序期间，Neurosuron会动态确定DNN的最佳分区点。 步骤如下：
  1. 分析并提取DNN架构的图层类型和配置；
  2. 系统使用已存的层性能预测模型来估计在移动和云上执行每一层的延迟和能量消耗；
  3. 通过这些预测，结合当前的无线连接带宽和数据中心负载水平，Neurosurgeon选择最佳分区点，优化最佳端到端延迟或最佳移动能耗；
  4. 执行DNN，在移动和云之间进行分区工作。



# 6. Evaluation

实验结果表明，相比于目前使用仅使用云服务器的方法，Neurosurgeon能够将应用的延迟时间平均降低了3.1倍（最高能达到40.7倍）。

在电量消耗方面，相比于现有方法，Neurosurgeon能够使得移动设备的电量消耗量平均降低至59.5%，最高能降低至94.7%。

下图展示了Neurosurgeon随着网络环境的变化（即LTE带宽变化）自适应进行分割和优化的结果（下图中的蓝色实线部分），可以看出比起现有方法（下图中的红色虚线部分）能够大幅度地降低延迟时间。

![](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/11.PNG)

Neurosurgeon也会与云服务器保持周期性的通讯，以获得其数据中心的负载情况。当服务器的数据中心负载较大时，它会减少往服务器上传输的数据量而增加移动设备本地的计算量。总之，Neurosurgeon能够根据服务器的负载情况作出适当的调整已达到最低的系统延迟时间。

![12](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/12.PNG)

随着连接网络质量的变差，Neurosurgeon会让移动设备承担更多的计算量，此时云服务器上数据中心的吞吐量将增加：较现有方法，连接LTE网络的情况下数据中心的吞吐量增加至1.43倍，而3G网络条件下则增加至2.36倍。此外还可以观察到，随着带GPU的移动设备百分比增加，Neurosurgeon增加了从云端到移动设备端的计算，导致更高的数据中心吞吐量改进。

![](https://cdn.jsdelivr.net/gh/xiangli-bjtu/Image-hosting-site/img/13.PNG)



# 个人体会

这个文章更多是一个偏实验性质的论文，但有着很好的指导意义。

1. 不少和MEC相关的综述文章，对于可分割计算任务，给出的是如下图这样很抽象的模型描述，这篇文章把计算任务具体到以DNN为核心的有关运算，能帮助理解这些抽象模型，实际上这也是边缘应用中非常普遍的计算任务之一（许多需要边缘计算的领域，需要运行神经网络为基础的应用程序）。

2. 作者给出了详细的实验设备、DNN框架、经典DNN模型的指导，应该来讲这篇论文的可复现的程度是很高的

3. 作者指出这种DNN为核心的计算任务，应该在移动端和边缘服务器上联合计算。对于计算量的划分，会受到DNN模型结构这一静态因素，和诸如无线网络状态、数据中心负载和设备剩余可用的电量等动态因素影响。对于动态因素在本文里多是以实验仿真去评估，作者实际并没有做很深入的数学上的分析。

4. 这类DNN为核心的计算任务，尤其是其中视觉有关的DNN模型。在工业场景下应该会存在更大的应用，如：车间摄像头实时监控流水线的设备运作画面、无人机航拍城市交通等，并通过无线网络上传画面配合边缘服务器联合计算。这类工业场景中的无线网络不能总保持良好状态，会影响计算结果的实时性（论文中指端到端时延，是否可以考虑引入信息年龄AoI的概念？）