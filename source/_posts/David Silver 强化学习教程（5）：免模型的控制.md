---
title: David Silver 强化学习教程（5）：免模型的控制
date: 2020-06-08 14:22:18
index_img: /img/post_index_img/david silver.jpg
math: true
tags: 
- Reinforcement Learning
- AI
categories: 技术
---

# Introduction

这一讲的内容主要是在模型未知的条件下如何优化价值函数，这一过程也称作模型无关的控制。

根据优化控制过程中是否利用已有经验策略来改进我们自身的控制策略，我们可以将这种优化控制分为两类：

**一类是同策略学习（On-policy Learning）**，其基本思想是个体已有一个策略，并且遵循这个策略进行采样，或者说采取一系列该策略下产生的行为，根据这一系列行为得到的奖励，更新状态函数，最后根据该更新的价值函数来优化策略得到较优的策略。这里，要优化的策略就是当前遵循的策略

**另一类是异策略学习（Off-policy Learning）**: 其基本思想是，虽然个体有一个自己的策略，但是个体并不针对这个策略进行采样，而是基于另一个策略进行采样，这另一个策略可以是先前学习到的策略，也可以是人类先验知识等一些较为优化成熟的策略，通过观察基于这类策略的行为，或者说通过对这类策略进行采样，得到这类策略下的各种行为，继而得到一些奖励，然后更新价值函数，即在自己的策略形成的价值函数的基础上观察别的策略产生的行为，以此达到学习的目的。



# On-Policy Monte-Carlo Control

在本节中我们使用的主要思想仍然是动态规划的思想。忘记了的不妨去看下第三讲回顾动态规划是如何进行策略迭代的。

那么这种方法是否适用于模型未知的蒙特卡洛学习呢？答案是否定的，这其中至少存在两个问题。

- 一是在模型未知的条件下无法知道当前状态的所有后续状态，进而无法确定在当前状态下采取怎样的行为更合适。（动态规划是需要知道某一状态的所有后续状态及状态间转移概率的）
- 另一个问题是，动态规划是采取的贪婪算法来改善策略，将很有可能由于没有足够的采样经验而导致产生一个并不是最优的策略。为了解决这一问题，我们需要引入一个随机机制，以一定的概率选择当前最好的策略，同时给以其它可能的行为一定的几率，这就是之前介绍强化学习时提到的$\epsilon$，数学表达式如下：

解决了上述两个问题，我们最终看到蒙特卡洛控制的全貌：**使用Ｑ函数进行策略评估，使用$\epsilon$-贪婪探索来改善策略**。该方法最终可以收敛至最优策略。如下图所示：

在实际求解控制问题时，为了使算法可以收敛，一般ϵϵ会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。这样我们可以得到一张和动态规划类似的图：



# On-Policy Temporal-Difference Control

上一讲提到TD相比MC有很多优点：

1. 低方差（lower variance）；
2. 可以在线实时学习；
3. 可以学习不完整episode等。

那么是否可以在控制问题上使用TD学习而不是MC学习？答案是肯定的，这就是下文要讲解的**SARSA算法**



### 总结

SARSA算法和动态规划法比起来，不需要环境的状态转换模型，和MC法比起来，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。但是也一个传统强化学习方法共有的问题，就是无法求解太复杂的问题，$Q(S,A)$使用一张大表来存储的，如果我们的状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA算还很不错的一种强化学习问题求解方法。



# Off-Policy Learning